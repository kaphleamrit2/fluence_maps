{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/pablojrios/fluence_maps/blob/master/tf2_evaluate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-n_PAVFGnZDx"
   },
   "source": [
    "# Compute predictions on a TF dataset using an stored .h5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hs99643-LfGS"
   },
   "outputs": [],
   "source": [
    "def isGoogleColab():\n",
    "    # 'ipykernel.zmqshell' runs in our server\n",
    "    # 'google.colab._shell' runs in Google Colab\n",
    "    return get_ipython().__class__.__module__ == 'google.colab._shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4LvLPLr5n_yJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "# Es indistinto usar las métricas de scikit learn o tensorflow\n",
    "# from sklearn.metrics import mean_absolute_error\n",
    "from tensorflow.keras.losses import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LN_LGyd6pWeZ",
    "outputId": "218786ce-f4b4-4c01-bf56-f33a17201821"
   },
   "outputs": [],
   "source": [
    "print('Tensorflow version = {}'.format(tf.__version__))\n",
    "print('Executing eagerly = {}'.format(tf.executing_eagerly()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JKOJC4VgLfGV",
    "outputId": "81a89d45-250f-4013-b8a6-52f7beb13ea5"
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "print('XGBoost version = {}'.format(xgb.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A85nWqMEnKPB",
    "outputId": "583cd467-f184-42b9-de8f-73d7043db7f7"
   },
   "outputs": [],
   "source": [
    "#%cd /content/drive/My\\ Drive/Healthcare/Radioterapia/data/ciolaplata\n",
    "#!unrar x /content/drive/My\\ Drive/Healthcare/Radioterapia/Mapas\\ CIO\\ La\\ Plata/Mapas\\ Calculados/2019.rar\n",
    "#!ls -l 2015/*dcm | wc -l\n",
    "\n",
    "if isGoogleColab():\n",
    "    %cd -q '/content'\n",
    "    if os.path.exists('fluence_maps'):\n",
    "        !rm -fr fluence_maps\n",
    "\n",
    "    GIT_USERNAME = \"pablojrios\"\n",
    "    GIT_TOKEN = \"1d88a0b85d2b00a03796e4d8b7e5f7b249b12f9b\"\n",
    "    !git clone -s https://{GIT_TOKEN}@github.com/{GIT_USERNAME}/fluence_maps.git\n",
    "\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    %cd -q '/content/fluence_maps'\n",
    "\n",
    "    !ls -l '/content/drive/My Drive/Healthcare/Radioterapia/data/ciolaplata/models'\n",
    "    \n",
    "    ARG_DATASET_DIR='/content/drive/My Drive/Healthcare/Radioterapia/data/ciolaplata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "krohuaX-Y6OZ"
   },
   "outputs": [],
   "source": [
    "from util.dataset import _tfrecord_dataset_type_from_folder, _parse_jpeg_image_function\n",
    "from util.preprocess import rescale_0_to_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eWlKtT6Kpbsr"
   },
   "outputs": [],
   "source": [
    "#============================DEFINE YOUR ARGUMENTS==============================\n",
    "if not isGoogleColab():\n",
    "    # base data directory\n",
    "    ARG_DATASET_DIR='/hdd/data/radioterapia/ciolaplata'\n",
    "# folder under ARG_DATASET_DIR path.\n",
    "ARG_RANDOM_SEED=23456\n",
    "ARG_TFDATASET_FOLDER=f'tfds.2019.localnorm.DS10%.{ARG_RANDOM_SEED}'\n",
    "# ARG_MODEL_NAME = '1612063529' # VGG16 20 de enero 2021\n",
    "ARG_MODEL_NAME = f'1612108487.{ARG_RANDOM_SEED}' # ene-31 ResNet18 buen(sobre) ajuste en training.\n",
    "# ARG_MODEL_NAME = '1611181684' # ResNet 18 20 de enero 2021\n",
    "PART_TRAIN = 'train'\n",
    "PART_VALIDATION = 'validation'\n",
    "PART_TEST = 'test'\n",
    "PART_TO_USE = PART_VALIDATION\n",
    "BATCH_SIZE = 32 # mae puede variar según batch size.\n",
    "num_workers = 8\n",
    "ARG_TRANSFORM_GAMMA=False\n",
    "ARG_TRAIN_XGBOOST=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K2-IdOEGqmU1"
   },
   "outputs": [],
   "source": [
    "def getMapDatasetForPartition(part):\n",
    "    tfdataset_dir = os.path.join(ARG_DATASET_DIR, ARG_TFDATASET_FOLDER)\n",
    "    raw_test = _tfrecord_dataset_type_from_folder(tfdataset_dir, part)\n",
    "    print(raw_test)\n",
    "\n",
    "    # Apply this function to each item in the dataset using the map method:\n",
    "    IMG_SIZE = 256\n",
    "    normalization_fn = rescale_0_to_1\n",
    "    test = raw_test.map(lambda e: _parse_jpeg_image_function(e, IMG_SIZE, normalization_fn, transform_gamma=ARG_TRANSFORM_GAMMA), num_parallel_calls=num_workers)\n",
    "    \n",
    "    gamma_values = test.map(lambda image, gamma, filename: gamma)\n",
    "    gamma_values = np.array(list(gamma_values.as_numpy_iterator()))\n",
    "    gamma_values[:20]\n",
    "    \n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DHBk0k2kLfGY",
    "outputId": "50dd327a-73c2-4e0e-a5f7-f5563dacf903"
   },
   "outputs": [],
   "source": [
    "map_dataset = getMapDatasetForPartition(PART_TO_USE)\n",
    "print(map_dataset)\n",
    "print(map_dataset.batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5TYrcou7LfGY"
   },
   "outputs": [],
   "source": [
    "def getBatchesForPartition(test, part):\n",
    "    test_batches = test.batch(BATCH_SIZE)\n",
    "    print(test_batches)\n",
    "\n",
    "    tmp_test_batches = test_batches.map(lambda image, gamma, filename: (image, gamma))\n",
    "    \n",
    "    return tmp_test_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tjrt80mNr0a4",
    "outputId": "cefccb7e-3227-4bfc-92d2-1eef253c60a0"
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "dir = os.path.join(ARG_DATASET_DIR, \"models\")\n",
    "saved_model_dir = '{}/{}.h5'.format(dir, ARG_MODEL_NAME)\n",
    "print(f'Loading model {saved_model_dir}...')\n",
    "loaded_model = tf.keras.models.load_model(saved_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lkz2ZGMDRNgC",
    "outputId": "390e00f4-e211-442b-8cd0-c92bd0989fdc"
   },
   "outputs": [],
   "source": [
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vwz1adqOLfGZ",
    "outputId": "9378f009-8dda-40f5-8c1a-9d39bf80a948"
   },
   "outputs": [],
   "source": [
    "tmp_test_batches = getBatchesForPartition(map_dataset, PART_TO_USE)\n",
    "print(tmp_test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3dPKkwTVLfGa"
   },
   "outputs": [],
   "source": [
    "def evaluatePartition(tmp_batches, part):\n",
    "    # Evaluate dataset with the loaded model to calculate loss (mae) because\n",
    "    # metric value could differ from the one reported during training.\n",
    "    loss, mse = loaded_model.evaluate(tmp_batches, workers=num_workers, verbose=0)\n",
    "    print('Loaded model, {} loss: {:5.4f}'.format(part, loss))\n",
    "    print('Loaded model, {} mse: {:5.4f}\\n'.format(part, mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F8cecGLVLfGa"
   },
   "outputs": [],
   "source": [
    "evaluatePartition(tmp_test_batches, PART_TO_USE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ctN7LhzY9I3n"
   },
   "outputs": [],
   "source": [
    "def getPredictionsDataframe(test, part):\n",
    "    tmp_batches = getBatchesForPartition(test, part)\n",
    "    # Make predictions\n",
    "    predictions = loaded_model.predict(tmp_batches)\n",
    "    # from (1121,1) to (1121,); ie.: ndim = 2 to ndim = 1\n",
    "    predictions = predictions.reshape(-1)\n",
    "    predictions.shape\n",
    "\n",
    "    tmp_test_dataset = test.map(lambda image, gamma, filename: (filename, gamma))\n",
    "    print(tmp_test_dataset)\n",
    "\n",
    "    lst = [(filename.numpy().decode('utf-8'), gamma.numpy()) for filename, gamma in tmp_test_dataset]\n",
    "    lst2 = [(e[0], e[1], p) for e, p in zip(lst, predictions)]\n",
    "\n",
    "    # armar un pandas dataframe con el test set completo\n",
    "    df = pd.DataFrame(lst2, columns=['filename', 'actual gamma', 'predicted gamma'])\n",
    "    dir = os.path.join(ARG_DATASET_DIR, \"predictions\")\n",
    "    predictions_file_path = '{}/predicted_gamma_{}.{}.csv'.format(dir, ARG_MODEL_NAME, part)\n",
    "    df.to_csv(predictions_file_path, index=False)\n",
    "    print(f'Predictions saved to {predictions_file_path}.\\n')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fcsyytskLfGb",
    "outputId": "1a64cd07-391a-4588-c4f2-3ef04a166c0c"
   },
   "outputs": [],
   "source": [
    "df = getPredictionsDataframe(map_dataset, PART_TO_USE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rc5oGykdLfGb"
   },
   "outputs": [],
   "source": [
    "def display_scatterplot(part, y, y_hat):\n",
    "    print('Set size = %d' % len(y))\n",
    "\n",
    "    # mean average error\n",
    "    mae = mean_absolute_error(y, y_hat)\n",
    "    print('MAE on %s = %.3f' % (part, mae))\n",
    "\n",
    "    # correlación entre predichos y observados\n",
    "    corr = np.corrcoef(y, y_hat)[0,1]\n",
    "    print('Corr. actual vs predicted = %.3f' % corr)\n",
    "\n",
    "    r2score = r2_score(y, y_hat)\n",
    "    print('R^2 = %.3f' % r2score)\n",
    "    \n",
    "    # plot calculados vs. predichos (targets)\n",
    "    fig_dims = (8, 8)\n",
    "    fig, ax = plt.subplots(figsize=fig_dims)\n",
    "    plt.title('Actual vs. Predicted (%s)' % part)\n",
    "    plt.gca().set_aspect('equal')\n",
    "    sns.scatterplot(y, y_hat, marker='o', ax=ax)\n",
    "    ax.set(xlabel=\"Actual\", ylabel = \"Predicted\")\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    minimum = int(np.min([y.min(), y_hat.min()]))\n",
    "    ax.set_xlim([minimum, 100])\n",
    "    ax.set_ylim([minimum, 100])\n",
    "    ax.plot([minimum, 100], [minimum, 100], ls=\"--\", c=\".7\")\n",
    "    ax.plot([minimum+3, 100], [minimum, 97], ls=\"--\", c=\".4\")\n",
    "    ax.plot([minimum, 97], [minimum+3, 100], ls=\"--\", c=\".4\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 660
    },
    "id": "e5envybQLfGc",
    "outputId": "e5f5259b-6e70-4eae-8e70-83733b1a9cac"
   },
   "outputs": [],
   "source": [
    "y = df['actual gamma']\n",
    "y_hat = df['predicted gamma']\n",
    "display_scatterplot(PART_TO_USE, y, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZU-BDT-FLfGd"
   },
   "source": [
    "## Train an XGBoost model with extracted CNN features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NwsDXo2sUvT_"
   },
   "outputs": [],
   "source": [
    "# For VGG16 only\n",
    "def extractFeatures(test, part):\n",
    "    tmp_test_batches = getBatchesForPartition(test, part)\n",
    "    print(tmp_test_batches)\n",
    "\n",
    "    inputs=loaded_model.get_layer(\"vgg16\").input\n",
    "    outputs=loaded_model.get_layer(\"vgg16\").output\n",
    "    cnn_part=tf.keras.models.Model(inputs, outputs)\n",
    "    predictions = cnn_part.predict(tmp_test_batches)\n",
    "\n",
    "    predictions[0][:,:,3].mean()\n",
    "\n",
    "    global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "    feature_batch_average = global_average_layer(predictions)\n",
    "    extracted_features = feature_batch_average.numpy()\n",
    "\n",
    "    from math import isclose\n",
    "    a=predictions[0][:,:,3].mean()\n",
    "    b=extracted_features[0][3]\n",
    "    isclose(a, b, rel_tol=1e-6)\n",
    "\n",
    "    return extracted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2a7cZmhRLfGe",
    "outputId": "883681b8-f139-41c2-82b3-2210c8ce7689"
   },
   "outputs": [],
   "source": [
    "extracted_features = extractFeatures(map_dataset, PART_TRAIN)\n",
    "extracted_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qIRjl9l0LfGe"
   },
   "outputs": [],
   "source": [
    "df_features = pd.DataFrame(extracted_features)\n",
    "df_result = pd.concat([df, df_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 431
    },
    "id": "HdfQ--vqLfGe",
    "outputId": "5a9f2b13-a48c-4a84-a1fb-b6f6158fe0b5"
   },
   "outputs": [],
   "source": [
    "part = PART_VALIDATION\n",
    "map_dataset = getMapDatasetForPartition(part)\n",
    "tmp_test_batches = getBatchesForPartition(map_dataset, part)\n",
    "evaluatePartition(tmp_test_batches, part)\n",
    "df_validation = getPredictionsDataframe(map_dataset, part)\n",
    "y = df_validation['actual gamma']\n",
    "y_hat = df_validation['predicted gamma']\n",
    "display_scatterplot(PART_VALIDATION, y, y_hat)\n",
    "extracted_features = extractFeatures(map_dataset, part)\n",
    "df_validation_features = pd.DataFrame(extracted_features)\n",
    "df_validation_result = pd.concat([df_validation, df_validation_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8BZgIQF6LfGf"
   },
   "outputs": [],
   "source": [
    "# training\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QNTgO9tNLfGf"
   },
   "outputs": [],
   "source": [
    "# validation\n",
    "df_validation_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tao_M8rZLfGf"
   },
   "outputs": [],
   "source": [
    "SEED = 12345\n",
    "\n",
    "X_train = df_features\n",
    "y_train = df['actual gamma']\n",
    "X_valid = df_validation_features\n",
    "y_valid = df_validation['actual gamma']\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_valid.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vfWEK84eLfGg"
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(data=X_train, label=y_train)\n",
    "dvalid = xgb.DMatrix(data=X_valid, label=y_valid)\n",
    "print(f'Number of features = {len(dtrain.feature_names)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ygxnl_f-LfGg"
   },
   "outputs": [],
   "source": [
    "def save_best_model(model_file):\n",
    "    best_score = float('inf')\n",
    "    \n",
    "    def callback(env):\n",
    "        nonlocal best_score\n",
    "        if env.evaluation_result_list[-1][1] < best_score:\n",
    "            env.model.save_model(model_file)\n",
    "            best_score = env.evaluation_result_list[-1][1]\n",
    "            \n",
    "    return callback;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NBa2IOSqLfGg"
   },
   "outputs": [],
   "source": [
    "# specify parameters via map, definition are same as c++ version: https://xgboost.readthedocs.io/en/latest/parameter.html or\n",
    "# https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst\n",
    "param = {'max_depth': 2\n",
    "         , 'eta': 0.1\n",
    "         , 'objective': 'reg:squarederror'\n",
    "         , 'eval_metric': ['rmse', 'mae']\n",
    "         , 'colsample_bytree': 0.3\n",
    "         # , 'colsample_bynode': 0.3\n",
    "         , 'lambda': 2\n",
    "         , 'alpha': 2\n",
    "         # , 'tree_method': 'gpu_hist' # usa la GPU\n",
    "         , 'seed': SEED\n",
    "         , 'min_child_weight': 5\n",
    "        }\n",
    "\n",
    "MODEL_FILE = 'best-xgboost.model'\n",
    "saved_model_dir = '{}/{}'.format(dir, MODEL_FILE)\n",
    "\n",
    "# specify validations set to watch performance\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "results = {}\n",
    "num_boost_round = 150\n",
    "bst = xgb.train(param, dtrain, num_boost_round, evals=watchlist, early_stopping_rounds=10, evals_result=results, callbacks=[save_best_model(saved_model_dir)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lEFCLeedLfGh"
   },
   "outputs": [],
   "source": [
    "epochs = len(results['train']['mae'])\n",
    "x_axis = range(0, epochs)\n",
    "# plot MAE\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_axis, results['train']['mae'], label='Training')\n",
    "ax.plot(x_axis, results['valid']['mae'], label='Validation')\n",
    "plt.axvline(bst.best_iteration, linestyle='--', color='k', label='best iteration')\n",
    "ax.legend()\n",
    "plt.ylabel('MAE')\n",
    "plt.xlabel('iteration')\n",
    "plt.title('XGBoost MAE')\n",
    "ymin=0; ymax=10\n",
    "plt.ylim(ymin, ymax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vCsKk89xLfGh"
   },
   "outputs": [],
   "source": [
    "print('best iteration: %i, best MAE on validation: %.3f' % (bst.best_iteration, bst.best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h1WiMTeKLfGh"
   },
   "outputs": [],
   "source": [
    "bst = xgb.Booster()  # init model\n",
    "bst.load_model(saved_model_dir)  # load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KeyHJOeeLfGi"
   },
   "outputs": [],
   "source": [
    "# This is prediction\n",
    "y_hat = bst.predict(dvalid)\n",
    "display_scatterplot(PART_VALIDATION, y_valid, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Zey_ms0PZK4"
   },
   "outputs": [],
   "source": [
    "if isGoogleColab():\n",
    "    drive.flush_and_unmount()\n",
    "    print('All changes made in this colab session should now be visible in Drive.')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "tf2_evaluate",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
