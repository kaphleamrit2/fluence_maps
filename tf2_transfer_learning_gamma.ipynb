{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/pablojrios/fluence_maps/blob/master/tf2_transfer_learning_gamma.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRTa3Ee15WsJ"
   },
   "source": [
    "# Transfer learning with a pretrained ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wRK8ctZQIEuc"
   },
   "outputs": [],
   "source": [
    "def isGoogleColab():\n",
    "    # 'ipykernel.zmqshell' runs in our server\n",
    "    # 'google.colab._shell' runs in Google Colab\n",
    "    return get_ipython().__class__.__module__ == 'google.colab._shell'\n",
    "\n",
    "#import sys\n",
    "#import IPython\n",
    "\n",
    "#if 'ipykernel' in sys.modules:\n",
    "#    ip = sys.modules['ipykernel']\n",
    "#    ip_version = ip.version_info\n",
    "#    ip_client = ip.write_connection_file.__module__.split('.')[0]\n",
    "\n",
    "#ip_version, ip_client\n",
    "\n",
    "#ip_version = IPython.utils.sysinfo.get_sys_info()['ipython_version']\n",
    "#ip_version\n",
    "\n",
    "#if 'IPython' in sys.modules:\n",
    "#    ip = sys.modules['IPython']\n",
    "#    ip_version = ip.version_info\n",
    "#    print(ip_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iBMcobPHdD8O"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import time\n",
    "from random import shuffle, randrange\n",
    "import random\n",
    "import tensorflow_addons as tfa\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "TqOt6Sv7AsMi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version = 2.2.0, addons version = 0.10.0\n",
      "Executing eagerly = True\n",
      "OpenCV version = 3.4.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'08/02/2021 23:51:22'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('Tensorflow version = {}, addons version = {}'.format(tf.__version__, tfa.__version__))\n",
    "print('Executing eagerly = {}'.format(tf.executing_eagerly()))\n",
    "keras = tf.keras\n",
    "\n",
    "import cv2 # to perform data augmentation\n",
    "print('OpenCV version = {}'.format(cv2.__version__))\n",
    "\n",
    "datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "q50x39yF5BPt"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "if isGoogleColab():\n",
    "    %cd '/content'\n",
    "    if os.path.exists('fluence_maps'):\n",
    "      !rm -fr fluence_maps\n",
    "\n",
    "    ## Install required dependencies\n",
    "    !pip install -q pydicom\n",
    "    ## to support ResNet18 and ResNet34\n",
    "    !pip install image-classifiers\n",
    "    ## https://github.com/tensorflow/addons/issues/2251\n",
    "    !pip install -U tensorflow-addons\n",
    "\n",
    "    GIT_USERNAME = \"pablojrios\"\n",
    "    GIT_TOKEN = \"1d88a0b85d2b00a03796e4d8b7e5f7b249b12f9b\"\n",
    "    !git clone -s https://{GIT_TOKEN}@github.com/{GIT_USERNAME}/fluence_maps.git\n",
    "\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    %cd -q '/content/fluence_maps'\n",
    "    \n",
    "    ARG_DATASET_DIR='/content/drive/My Drive/Healthcare/Radioterapia/data/ciolaplata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LhWAVjltIJWh"
   },
   "outputs": [],
   "source": [
    "# To support ResNet18 and ResNet34 for tensorflow.keras\n",
    "from classification_models.tfkeras import Classifiers\n",
    "from util.preprocess import rescale_0_to_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(Enum):\n",
    "        vgg16 = 1\n",
    "        resnet18 = 2\n",
    "        resnet34 = 3\n",
    "        mobilenetV2 = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "uFuIiQp2zVUF"
   },
   "outputs": [],
   "source": [
    "# ===============================================DEFINE YOUR ARGUMENTS=================================================\n",
    "if not isGoogleColab():\n",
    "    ARG_DATASET_DIR='/hdd/data/radioterapia/ciolaplata'\n",
    "# folder under ARG_DATASET_DIR path.\n",
    "ARG_RANDOM_SEED = 23456\n",
    "# ARG_TFDATASET_FOLDER=f'tfds.2018-2019.localnorm.DS10%.{ARG_RANDOM_SEED}.fold0'\n",
    "ARG_TFDATASET_FOLDER=f'tfds.2019-2017.localnorm.DS10%.{ARG_RANDOM_SEED}.fold4'\n",
    "# if False only training and validation partition are created.\n",
    "ARG_TEST_PARTITION=False\n",
    "ARG_NETWORK=CNN.resnet18\n",
    "ARG_RESNET_USE_BN=True\n",
    "\n",
    "# number of continuous epochs without improvement on validation\n",
    "ARG_EPOCHS_WO_IMPROVEMENT=40 # 20 for 2019 only, 10 for 2019+2018, 30 for 2019+2018 for MobileNetV2\n",
    "initial_epochs = 10 # 10 for 2019 only, 5 for 2019+2018\n",
    "# maximum fine-tuning epochs\n",
    "ARG_MAX_FINE_TUNING_EPOCHS=200\n",
    "# 0: use custom LR, 1: use ReduceLROnPlateau\n",
    "ARG_LR_SCHEDULE=1\n",
    "ARG_LR_PATIENCE=20 # only applies if ARG_LR_SCHEDULE is 1. Equals to 10 except for MobileNetV2 that requires 15+\n",
    "ARG_MIN_DELTA_MAE=0.01\n",
    "base_learning_rate = 0.001\n",
    "\n",
    "ARG_DATA_AUGMENTATION=False\n",
    "# perform data augmentation of images with a gamma value lower than gamma_augment\n",
    "ARG_GAMMA_AUGMENT=97.0\n",
    "# set this value based on the ARG_OVERSAMPLING_FACTOR value in tf2_oversampling_dicom_files.py\n",
    "# 1.0 means transform each and every image, with a lower value than 1.0 means that training will include unmodified (not transformed)\n",
    "# images.\n",
    "ARG_AUGMENT_PROBABILITY=0.75 # con 0.85 para el dataset tfds.2019.localnorm.ovs95x8.0 no transformo 93*(1+8)*0.15=125 imágenes  \n",
    "add_regularizers=False\n",
    "## Fine-tune from this layer onwards\n",
    "# fine_tune_at = 281 # InceptionV3, fine-tuning\n",
    "# fine_tune_at = 102 # InceptionV3, not so fine-tuning\n",
    "# fine_tune_at = 74 # resnet34 stage3\n",
    "# fine_tune_at = 129 # resnet34 stage4\n",
    "if ARG_NETWORK == CNN.vgg16:\n",
    "    fine_tune_at = 11 # VGG16\n",
    "elif ARG_NETWORK == CNN.resnet18:\n",
    "    fine_tune_at = 46 # resnet18 stage3\n",
    "elif ARG_NETWORK == CNN.resnet34:\n",
    "    fine_tune_at = 73 # resnet34 stage3\n",
    "else:\n",
    "    fine_tune_at = 117 # MobileNetV2 block_12_add para atrás\n",
    "ARG_TRANSFORM_GAMMA=False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v77rlkCKW0IJ"
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0GoKGm1duzgk"
   },
   "source": [
    "### Data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "KVh7rDVAuW8Y"
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "tf.random.set_seed(ARG_RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1KR9xb8NyFTC"
   },
   "outputs": [],
   "source": [
    "def _tfrecord_dataset_type_from_folder(folder, dataset_type, ext='.tfrecords'):\n",
    "    tfrecords = [os.path.join(folder, n)\n",
    "                 for n in os.listdir(folder) if n.startswith(dataset_type) and n.endswith(ext)]\n",
    "    return tf.data.TFRecordDataset(tfrecords)\n",
    "\n",
    "tfdataset_dir = os.path.join(ARG_DATASET_DIR, ARG_TFDATASET_FOLDER)\n",
    "# type(raw_train) is tensorflow.python.data.ops.readers.TFRecordDatasetV2\n",
    "raw_train = _tfrecord_dataset_type_from_folder(tfdataset_dir, 'train')\n",
    "raw_validation = _tfrecord_dataset_type_from_folder(tfdataset_dir, 'validation')\n",
    "if ARG_TEST_PARTITION:\n",
    "    raw_test = _tfrecord_dataset_type_from_folder(tfdataset_dir, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o29EfE-p0g5X"
   },
   "source": [
    "The resulting `tf.data.Dataset` objects contain `(image, label)` pairs where the images have variable shape and 3 channels, and the label is a scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "GIys1_zY1S9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TFRecordDatasetV2 shapes: (), types: tf.string>\n",
      "<TFRecordDatasetV2 shapes: (), types: tf.string>\n"
     ]
    }
   ],
   "source": [
    "print(raw_train)\n",
    "print(raw_validation)\n",
    "if ARG_TEST_PARTITION:\n",
    "    print(raw_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "T1Gm1wdHyFTK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in training part = 3415.\n",
      "Number of images in validation part = 853.\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of images in training part = {sum(1 for _ in raw_train)}.')\n",
    "print(f'Number of images in validation part = {sum(1 for _ in raw_validation)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ar8TNzTSyFTS"
   },
   "outputs": [],
   "source": [
    "def _parse_image_function(example_proto, img_size, normalization_fn, data_augmentation=False, augment_probability=1.0, transform_gamma=False):\n",
    "    # Create a dictionary describing the features.\n",
    "    image_feature_description = {\"image/filename\": tf.io.FixedLenFeature((), tf.string),\n",
    "                \"image/encoded\": tf.io.FixedLenFeature((), tf.string),\n",
    "                \"image/format\": tf.io.FixedLenFeature((), tf.string),\n",
    "                \"image/gamma_index\": tf.io.FixedLenFeature((), tf.float32),\n",
    "                \"image/height\": tf.io.FixedLenFeature((), tf.int64),\n",
    "                \"image/width\": tf.io.FixedLenFeature((), tf.int64)}\n",
    "    \n",
    "    def image_augment(image):\n",
    "        radian = ((np.random.random()-.50)*10 / 360) * np.pi\n",
    "        # tfa.image.transform_ops.rotate(image, radian) is an alias\n",
    "        image = tfa.image.rotate(image, radian)\n",
    "        # image = tf.image.random_flip_left_right(image)\n",
    "        image = random_translation(image)\n",
    "        return image\n",
    "\n",
    "    # Now, globally set everything to run eagerly\n",
    "    # The following doesn't set to eager mode:\n",
    "    # UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set,\n",
    "    # this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
    "    # tf.config.run_functions_eagerly(True)\n",
    "\n",
    "    # Executing eagerly = False here!\n",
    "    # numpy is only supported in eager mode. If you are in graph mode, it will not be supported.\n",
    "    # In eager execution the shape is always fully-known.\n",
    "    # print('Executing eagerly = {}'.format(tf.executing_eagerly()))\n",
    "\n",
    "    # Parse the input tf.Example proto using the dictionary above.\n",
    "    parsed = tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "    \n",
    "    image = tf.image.decode_jpeg(parsed[\"image/encoded\"], channels=3)\n",
    "    # print(type(image), image.shape, image.dtype) # <class 'tensorflow.python.framework.ops.Tensor'> (None, None, 3) <dtype: 'uint8'>\n",
    "\n",
    "    gamma = tf.cast(\n",
    "        parsed[\"image/gamma_index\"],\n",
    "        tf.float32)\n",
    "    # print(type(gamma), gamma.shape, gamma.dtype) # <class 'tensorflow.python.framework.ops.Tensor'> () <dtype: 'float32'>\n",
    "\n",
    "    image = normalization_fn(image)\n",
    "\n",
    "    image = tf.image.resize(image, (img_size, img_size))\n",
    "    print(type(image), image.shape, image.dtype)\n",
    "\n",
    "    if data_augmentation and augment_probability >= random.uniform(0, 1):\n",
    "        gamma_augment = tf.constant(ARG_GAMMA_AUGMENT)\n",
    "        image = tf.cond(tf.math.less(gamma, gamma_augment)\n",
    "                        , lambda: image_augment(image)\n",
    "                        , lambda: image)\n",
    "\n",
    "    # normalizo antes de transformar\n",
    "    # image = normalization_fn(image)\n",
    "    \n",
    "    #label is a tensor of an array of single tf.int64 arrays.\n",
    "    #label = tf.cast(\n",
    "    #    tf.reshape(parsed[\"image/class/label\"], [-1]),\n",
    "    #    tf.int64)\n",
    "\n",
    "    # assert tf.executing_eagerly() FAILS\n",
    "    # parsed[\"image/filename\"] is a Tensor and not an EagerTensor because we are in a map function,\n",
    "    # because in 2.0, code inside Datasets maps is turned into a subgraph for speed, just as it was in 1.x eager\n",
    "    # execution. You generally want to avoid Python inside your data pipeline.\n",
    "    # So, if I invoke parsed[\"image/filename\"].numpy().decode('utf-8') to get the filename string the error\n",
    "    # \"AttributeError: 'Tensor' object has no attribute 'numpy'\" is thrown, hence I return a tensor.\n",
    "    filename = parsed[\"image/filename\"]\n",
    "\n",
    "    if transform_gamma:\n",
    "        gamma = 60.0 - (105 - gamma)\n",
    "                    \n",
    "    return image, gamma, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "CyTNIMaeyFTX"
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 256 # All images will be resized to 256x256\n",
    "if ARG_NETWORK == CNN.mobilenetV2: IMG_SIZE = 224\n",
    "\n",
    "normalization_fn = rescale_0_to_1 # rescale_min_1_to_1\n",
    "# normalization_fn = tf.image.per_image_standardization # loss y mae en validación reportan números muy grandes,\n",
    "# no así en training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "SFZ6ZW7KSXP9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.Tensor'> (256, 256, 3) <dtype: 'float32'>\n",
      "<class 'tensorflow.python.framework.ops.Tensor'> (256, 256, 3) <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "num_workers = 8\n",
    "\n",
    "# assert tf.executing_eagerly()\n",
    "if ARG_DATA_AUGMENTATION:\n",
    "    print(\"Training with image augmentation.\")\n",
    "    \n",
    "train = raw_train.map(lambda e: _parse_image_function(e, IMG_SIZE, normalization_fn, ARG_DATA_AUGMENTATION, ARG_AUGMENT_PROBABILITY,\n",
    "                                                      transform_gamma=ARG_TRANSFORM_GAMMA),\n",
    "                      num_parallel_calls=num_workers)\n",
    "validation = raw_validation.map(lambda e: _parse_image_function(e, IMG_SIZE, normalization_fn, transform_gamma=ARG_TRANSFORM_GAMMA),\n",
    "                                num_parallel_calls=num_workers)\n",
    "if ARG_TEST_PARTITION:\n",
    "    test = raw_test.map(lambda e: _parse_image_function(e, IMG_SIZE, normalization_fn),\n",
    "                        num_parallel_calls=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Yic-I66m6Isv"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER_SIZE = 1000\n",
    "PREFETCH_BUFFER_SIZE = 2 * BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "p3UUPdm86LNC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing eagerly = True\n",
      "<PrefetchDataset shapes: ((None, 256, 256, 3), (None,), (None,)), types: (tf.float32, tf.float32, tf.string)>\n",
      "<PrefetchDataset shapes: ((None, 256, 256, 3), (None,), (None,)), types: (tf.float32, tf.float32, tf.string)>\n"
     ]
    }
   ],
   "source": [
    "print('Executing eagerly = {}'.format(tf.executing_eagerly()))\n",
    "\n",
    "# <PrefetchDataset shapes: ((None, 224, 224, 3), (None, 1)), types: (tf.float32, tf.int64)>\n",
    "train_batches = train.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE).prefetch(PREFETCH_BUFFER_SIZE)\n",
    "print(train_batches)\n",
    "validation_batches = validation.batch(BATCH_SIZE).prefetch(PREFETCH_BUFFER_SIZE)\n",
    "print(validation_batches)\n",
    "# <BatchDataset shapes: ((None, 224, 224, 3), (None, 1)), types: (tf.float32, tf.int64)>\n",
    "if ARG_TEST_PARTITION:\n",
    "    test_batches = test.batch(BATCH_SIZE)\n",
    "    print(test_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02rJpcFtChP0"
   },
   "source": [
    "Inspect a batch of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "iknFo3ELBVho"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([32, 256, 256, 3]), TensorShape([32]), TensorShape([32]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for image_batch, label_batch, filename_batch in train_batches.take(1):\n",
    "    pass\n",
    "\n",
    "image_batch.shape, label_batch.shape, filename_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "5pIy5ehM4YzC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma=98.92, filename=/hdd/data/radioterapia/ciolaplata/2019/1.3.6.1.4.1.9590.100.1.2.344598092513588358038786553670066783277.dcm\n",
      "image shape = (256, 256, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display 2nd image in the batch\n",
    "gamma_value = label_batch[8].numpy()\n",
    "filename = filename_batch[8].numpy().decode('utf-8')\n",
    "print(f'gamma={gamma_value:2.2f}, filename={filename}')\n",
    "# if pixel values are float they have to be in [0, 1] range, if they are integer they have to be in the [0, 255] range,\n",
    "# else pixel values are truncated.\n",
    "im = image_batch[8].numpy()\n",
    "plt.imshow(im)\n",
    "print(\"image shape = {}\".format(im.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im[:,:,0].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OkH-kazQecHB"
   },
   "source": [
    "## Create the base model from the pre-trained convnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "aiLcQ7JkDM6U"
   },
   "outputs": [],
   "source": [
    "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
    "\n",
    "if ARG_NETWORK==CNN.resnet18:\n",
    "    ResNet18, preprocess_input = Classifiers.get('resnet18')\n",
    "    base_model = ResNet18(input_shape=IMG_SHAPE, weights='imagenet', include_top=False)\n",
    "    # no transfer learning\n",
    "    # base_model = ResNet18(input_shape=IMG_SHAPE, include_top=False)\n",
    "\n",
    "elif ARG_NETWORK==CNN.resnet34:\n",
    "    ResNet34, preprocess_input = Classifiers.get('resnet34')\n",
    "    base_model = ResNet34(input_shape=IMG_SHAPE, weights='imagenet', include_top=False)\n",
    "\n",
    "elif ARG_NETWORK== CNN.mobilenetV2:\n",
    "    ## Create the base model from the pre-trained model MobileNet V2\n",
    "    base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                                   # alpha=1.4,\n",
    "                                                   weights='imagenet',\n",
    "                                                   include_top=False)\n",
    "\n",
    "else:\n",
    "    base_model = tf.keras.applications.VGG16(input_shape=IMG_SHAPE,\n",
    "                                                   weights='imagenet',\n",
    "                                                   include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqcsxoJIEVXZ"
   },
   "source": [
    "This feature extractor converts each `160x160x3` image into a `5x5x1280` block of features. See what it does to the example batch of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Y-2LJL0EEUcx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 8, 8, 512)\n"
     ]
    }
   ],
   "source": [
    "feature_batch = base_model(image_batch)\n",
    "print(feature_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rlx56nQtfe8Y"
   },
   "source": [
    "## Feature extraction\n",
    "In this step, you will freeze the convolutional base created from the previous step and to use as a feature extractor. Additionally, you add a classifier on top of it and train the top-level classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CnMLieHBCwil"
   },
   "source": [
    "### Freeze the convolutional base\n",
    "\n",
    "It is important to freeze the convolutional base before you compile and train the model. Freezing (by setting layer.trainable = False) prevents the weights in a given layer from being updated during training. MobileNet V2 has many layers, so setting the entire model's trainable flag to False will freeze all the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OTCJH4bphOeo"
   },
   "outputs": [],
   "source": [
    "if not (ARG_NETWORK == CNN.resnet18 or ARG_NETWORK == CNN.resnet34):\n",
    "    base_model.trainable = False\n",
    "else:\n",
    "    # resnet\n",
    "    for layer in base_model.layers:\n",
    "            if layer.__class__.__name__ != \"BatchNormalization\":\n",
    "                layer.trainable = False\n",
    "            else:\n",
    "                print(f\"{layer.name} ({layer.__class__.__name__})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KpbzSmPkDa-N"
   },
   "outputs": [],
   "source": [
    "# Let's take a look at the base model architecture\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7RFyBW06yFUC"
   },
   "outputs": [],
   "source": [
    "from add_regularization import add_regularization\n",
    "# adds a tf.keras.regularizers.l2(0.0001)\n",
    "#\"kernel_regularizer\":{\n",
    "#                        \"class_name\": \"L1L2\",\n",
    "#                        \"config\": {\n",
    "#                            \"l1\": 0,\n",
    "#                            \"l2\": 0.0001\n",
    "#                        }\n",
    "#                     }\n",
    "if add_regularizers:\n",
    "    base_model = add_regularization(base_model)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    for attr in ['kernel_regularizer']:\n",
    "        if hasattr(layer, attr):\n",
    "            if getattr(layer, attr) is None:\n",
    "                print('layer {} has no regularizer.'.format(layer.name))\n",
    "            else:\n",
    "                print('layer {} has a regularizer {}.'.format(layer.name, getattr(layer, attr)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "avX9AnbJyFUH"
   },
   "outputs": [],
   "source": [
    "# display the weights of some layers\n",
    "l = 1\n",
    "for layer in base_model.layers:\n",
    "    # print('layer {:3d}, name: {}'.format(l, layer.name))\n",
    "    if layer.name == \"expanded_conv_project\":\n",
    "        weights = layer.get_weights()\n",
    "        print(layer.get_config(), weights, weights[0].shape)\n",
    "    l += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "dLnpMF5KOALm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 512)\n"
     ]
    }
   ],
   "source": [
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "feature_batch_average = global_average_layer(feature_batch)\n",
    "print(feature_batch_average.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JzmSozfKDM6W"
   },
   "source": [
    "Stack the feature extractor, and these two layers using a `tf.keras.Sequential` model for network architectures other than ResNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Wv4afXKj6cVa"
   },
   "outputs": [],
   "source": [
    "num_activation_filters = 512\n",
    "if ARG_NETWORK == CNN.mobilenetV2:\n",
    "    num_activation_filters = 1280\n",
    "\n",
    "if not (ARG_NETWORK == CNN.resnet18 or ARG_NETWORK == CNN.resnet34):\n",
    "    # Pablo March 10: add sigmoid\n",
    "    # WARNING: adding the activation function causes loss to keep close to 0.5 and does not decrease.\n",
    "    # prediction_layer = keras.layers.Dense(1, activation='sigmoid') # para obtener probabilidades y no logits\n",
    "\n",
    "    # https://keras.io/applications/#fine-tune-inceptionv3-on-a-new-set-of-classes\n",
    "    # https://stackoverflow.com/questions/58627411/how-to-use-inception-network-for-regression\n",
    "\n",
    "    # let's add a fully-connected layer\n",
    "    fc1 = keras.layers.Dense(num_activation_filters, activation='relu')\n",
    "    #fc2 = keras.layers.Dense(num_activation_filders, activation='relu')\n",
    "    #dropout = keras.layers.Dropout(rate=0.05) # no funciona\n",
    "    # and a linear output layer (regression)\n",
    "\n",
    "    bn = keras.layers.BatchNormalization() # same as ResNet18 but with VGG16 is worst.\n",
    "\n",
    "    prediction_layer = keras.layers.Dense(1, activation='linear')\n",
    "    # and a logistic layer -- let's say we have 200 classes (classification)\n",
    "    # prediction_layer = Dense(200, activation='softmax')(x)\n",
    "\n",
    "    # Up to March 27, 2020\n",
    "    # prediction_layer = keras.layers.Dense(1)\n",
    "    prediction_batch = prediction_layer(feature_batch_average)\n",
    "    print(prediction_batch.shape)\n",
    "\n",
    "    # Now stack the feature extractor, and these two layers using a `tf.keras.Sequential` model:\n",
    "    model = tf.keras.Sequential([\n",
    "      base_model,\n",
    "      global_average_layer,\n",
    "      fc1,\n",
    "      #bn,\n",
    "      #dropout,\n",
    "      #fc2,\n",
    "      prediction_layer\n",
    "    ])\n",
    "    \n",
    "else:\n",
    "    avg = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "    out = tf.keras.layers.Dense(num_activation_filters, activation=\"relu\")(avg) # CHANGE to use avg pool only or avg pool + max pool\n",
    "    if ARG_RESNET_USE_BN:\n",
    "        # no observé mejoras en 2017 agregando esta bn layer, pero sí observé mejoras en 4 de 5 folds de cross validation\n",
    "        # en 2019+2017\n",
    "        out = tf.keras.layers.BatchNormalization()(out)\n",
    "    prediction_layer = tf.keras.layers.Dense(1, activation='linear')(out)\n",
    "    model = keras.models.Model(inputs=base_model.input, outputs=prediction_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0ylJXE_kRLi"
   },
   "source": [
    "### Compile the model\n",
    "\n",
    "You must compile the model before training it.  Since there are two classes, use a binary cross-entropy loss with `from_logits=True` since the model provides a linear output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RpR8HdyMhukJ"
   },
   "outputs": [],
   "source": [
    "# mse = square(y_true - y_pred)\n",
    "# mae = loss = abs(y_true - y_pred)\n",
    "# mape = 100 * abs(y_true - y_pred) / y_true\n",
    "# mae y mape son similares, no iguales, por eso tomo MAE que es el promedio de la diferencia absoluta entre el\n",
    "# gamma real y el gamma predicho\n",
    "# optimizer = tf.keras.optimizers.RMSprop(lr=base_learning_rate, momentum=0.95)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=base_learning_rate)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.MeanAbsoluteError(),\n",
    "              metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I8ARiyMFsgbH"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RxvgOYTDSWTx"
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "hlHEavK7DUI7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in train partition: 3415, in validation: 853.\n"
     ]
    }
   ],
   "source": [
    "num_train = sum(1 for _ in raw_train)\n",
    "num_val = sum(1 for _ in raw_validation)\n",
    "print(f'Number of images in train partition: {num_train}, in validation: {num_val}.')\n",
    "if ARG_TEST_PARTITION:\n",
    "    num_test = sum(1 for _ in raw_test)\n",
    "    print(f'Number of images in test partiton: {num_test}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Om4O3EESkab1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MapDataset shapes: ((None, 256, 256, 3), (None,)), types: (tf.float32, tf.float32)>\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = round(num_train)//BATCH_SIZE\n",
    "validation_steps=20\n",
    "\n",
    "# projects out just the first two components.\n",
    "tmp_validation_batches = validation_batches.map(lambda image, gamma, filename: (image, gamma))\n",
    "print(tmp_validation_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8cYT1c48CuSd"
   },
   "outputs": [],
   "source": [
    "loss0 = mse0 = 0\n",
    "loss0, mse0 = model.evaluate(tmp_validation_batches, steps = validation_steps)\n",
    "\n",
    "print(\"initial loss: {:.2f}\".format(loss0))\n",
    "print(\"initial mape: {:.2f}\".format(mse0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "JsaRFlZ9B6WK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MapDataset shapes: ((None, 256, 256, 3), (None,)), types: (tf.float32, tf.float32)>\n"
     ]
    }
   ],
   "source": [
    "# projects out just the first two components.\n",
    "tmp_train_batches = train_batches.map(lambda image, gamma, filename: (image, gamma))\n",
    "print(tmp_train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "XTtp6LG7yFUw"
   },
   "outputs": [],
   "source": [
    "# Implement callback function to stop training\n",
    "class MyCallback(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, wait_epochs):\n",
    "        self.__wait_epochs = wait_epochs\n",
    "        self.__latest_peak_mae = 50.0 # MobileNetV2 requires such a high value because it improves very slowly.\n",
    "        self.__waited_epochs = 0\n",
    "        self.__saved_model_file = None\n",
    "        self.__saved_model = None\n",
    "        \n",
    "    def stopTraining(self, epoch, val_mae):\n",
    "        stop_early = False\n",
    "        best_mae = False\n",
    "        # check for early stop\n",
    "        if val_mae < self.__latest_peak_mae and abs(val_mae - self.__latest_peak_mae) > ARG_MIN_DELTA_MAE:\n",
    "            best_mae = True\n",
    "            self.__latest_peak_mae = val_mae\n",
    "            print(f\"\\nNew peak val_mae reached: {val_mae:6.3}\")\n",
    "\n",
    "            t = time.time()\n",
    "            dir = os.path.join(ARG_DATASET_DIR, \"models\")\n",
    "            save_model_path = \"{}/{}.{}.{}.h5\".format(dir, int(t), ARG_NETWORK.name, ARG_RANDOM_SEED)\n",
    "            print(save_model_path)\n",
    "            # Either 'tf' or 'h5', indicating whether to save the model to Tensorflow SavedModel or HDF5. Defaults to 'tf' \n",
    "            # in TF 2.X, and 'h5' in TF 1.X.            \n",
    "            model.save(save_model_path, save_format='h5')\n",
    "            # borro el archivo del modelo anterior\n",
    "            if self.__saved_model_file is not None:\n",
    "                os.remove(self.__saved_model_file)\n",
    "            self.__saved_model_file = save_model_path\n",
    "            reloaded_model = tf.keras.models.load_model(save_model_path)\n",
    "            self.__saved_model = reloaded_model\n",
    "\n",
    "        if val_mae > self.__latest_peak_mae or not best_mae:\n",
    "            # Si llevo N+ epochs sin mejora\n",
    "            if self.__waited_epochs >= self.__wait_epochs:\n",
    "                print(\"\\nStopping early at epoch {0} with saved peak mae {1:10.8}\"\n",
    "                      .format(epoch + 1, self.__latest_peak_mae))\n",
    "                stop_early = True\n",
    "            \n",
    "            self.__waited_epochs += 1\n",
    "            \n",
    "        else:\n",
    "            self.__latest_peak_mae = val_mae\n",
    "            # Reset waited epochs.\n",
    "            print(\"waiting epochs reset.\")\n",
    "            self.reset_waited_epochs()\n",
    "            \n",
    "        return stop_early\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # print('\\nTraining: epoch {} ends at {}'.format(epoch, datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")))\n",
    "        if self.stopTraining(epoch, logs.get('val_loss')):\n",
    "            self.model.stop_training = True\n",
    "\n",
    "    @property\n",
    "    def saved_model_file(self):\n",
    "        return self.__saved_model_file\n",
    "    \n",
    "    @property\n",
    "    def saved_model(self):\n",
    "        return self.__saved_model\n",
    "\n",
    "    def reset_waited_epochs(self):\n",
    "        self.__waited_epochs = 0;\n",
    "    \n",
    "\n",
    "# Instantiate a callback object\n",
    "callbackObj = MyCallback(ARG_EPOCHS_WO_IMPROVEMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MZyPn887yFUz"
   },
   "outputs": [],
   "source": [
    "history = model.fit(tmp_train_batches,\n",
    "                    epochs=initial_epochs,\n",
    "                    validation_data=tmp_validation_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hd94CKImf8vi"
   },
   "source": [
    "### Learning curves\n",
    "\n",
    "Let's take a look at the learning curves of the training and validation accuracy/loss when using the MobileNet V2 base model as a fixed feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zvmYWOgoyFU4"
   },
   "outputs": [],
   "source": [
    "print(history.epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Po3M6UrGvKr"
   },
   "outputs": [],
   "source": [
    "# Pablo Feb 25:\n",
    "# Python never implicitly copies objects. When you set list2 = list1, you are making them refer to the same exact\n",
    "# list object, so when you mutate it, all references to it keep referring to the object in its current state.\n",
    "mse = history.history['mse'].copy()\n",
    "val_mse = history.history['val_mse'].copy()\n",
    "\n",
    "loss = history.history['loss'].copy()\n",
    "val_loss = history.history['val_loss'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "53OTCh3jnbwV"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(mse, label='Training MSE')\n",
    "plt.plot(val_mse, label='Validation MSE')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('MSE')\n",
    "plt.ylim([0,50])\n",
    "plt.title('Training and Validation MSE')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim([0,10])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CqwV-CRdS6Nv"
   },
   "source": [
    "## Fine tuning\n",
    "In the feature extraction experiment, you were only training a few layers on top of a VGG16 base model. The weights of the pre-trained network were **not** updated during training.\n",
    "\n",
    "One way to increase performance even further is to train (or \"fine-tune\") the weights of the top layers of the pre-trained model alongside the training of the classifier you added. The training process will force the weights to be tuned from generic feature maps to features associated specifically with the dataset.\n",
    "\n",
    "Note: This should only be attempted after you have trained the top-level classifier with the pre-trained model set to non-trainable. If you add a randomly initialized classifier on top of a pre-trained model and attempt to train all layers jointly, the magnitude of the gradient updates will be too large (due to the random weights from the classifier) and your pre-trained model will forget what it has learned.\n",
    "\n",
    "Also, you should try to fine-tune a small number of top layers rather than the whole VGG16 model. In most convolutional networks, the higher up a layer is, the more specialized it is. The first few layers learn very simple and generic features that generalize to almost all types of images. As you go higher up, the features are increasingly more specific to the dataset on which the model was trained. The goal of fine-tuning is to adapt these specialized features to work with the new dataset, rather than overwrite the generic learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CPXnzUK0QonF"
   },
   "source": [
    "### Un-freeze the top layers of the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfxv_ifotQak"
   },
   "source": [
    "All you need to do is unfreeze the `base_model` and set the bottom layers to be un-trainable. Then, you should recompile the model (necessary for these changes to take effect), and resume training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4nzcagVitLQm"
   },
   "outputs": [],
   "source": [
    "base_model.trainable = True\n",
    "\n",
    "# Pablo Feb 25: con este loop me aseguro que las capas que en la próxima celda *NO* pongo como layer.trainable = False sean\n",
    "# trainable.\n",
    "l = 1\n",
    "for layer in base_model.layers:\n",
    "    print('layer {:3d}, name: {}'.format(l, layer.name))\n",
    "    if layer.__class__.__name__ != \"BatchNormalization\":\n",
    "        layer.trainable =  True\n",
    "    l += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-4HgVAacRs5v"
   },
   "outputs": [],
   "source": [
    "# Let's take a look to see how many layers are in the base model\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "l = 1\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    if layer.__class__.__name__ != \"BatchNormalization\":\n",
    "        print('layer {:3d}, name: {} set to not trainable.'.format(l, layer.name))\n",
    "        layer.trainable = False\n",
    "    l += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Uk1dgsxT0IS"
   },
   "source": [
    "### Compile the model\n",
    "\n",
    "Compile the model using a much lower learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "AVvlrXb9yFVH"
   },
   "outputs": [],
   "source": [
    "# This function keeps the learning rate at 0.001 for the first ten epochs\n",
    "# and decreases it exponentially after that.\n",
    "def lr_scheduler(epoch):\n",
    "    lr = base_learning_rate\n",
    "    #k = 0.15\n",
    "    #if epoch >= 10 and epoch <= 40:\n",
    "    #    lr = tf.math.exp(k * (10 - epoch)) * base_learning_rate\n",
    "    #    print(\"\\nlearning rate: %.8f\"%(lr))\n",
    "    if epoch >= 10 and epoch <= 30:\n",
    "        lr = base_learning_rate/10\n",
    "    elif epoch > 30 and epoch <= 60:\n",
    "        lr = base_learning_rate/100\n",
    "    elif epoch > 60:\n",
    "        lr = base_learning_rate/500\n",
    "    print(\"\\nlearning rate: %.6f\"%(lr))\n",
    "    \n",
    "    return lr\n",
    "    \n",
    "    \n",
    "callbackLR = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "MvJPcR-PIEu2"
   },
   "outputs": [],
   "source": [
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=ARG_LR_PATIENCE, verbose=1, min_delta=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "NtUnaz0WUDva"
   },
   "outputs": [],
   "source": [
    "# optimizer = tf.keras.optimizers.RMSprop(lr=base_learning_rate/10, momentum=0.95)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=base_learning_rate/10, epsilon=1e-08, amsgrad=False)\n",
    "model.compile(loss=tf.keras.losses.MeanAbsoluteError(),\n",
    "              optimizer = optimizer,\n",
    "              metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "WwBWy7J2kZvA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "data (InputLayer)               [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bn_data (BatchNormalization)    (None, 256, 256, 3)  9           data[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2D)  (None, 262, 262, 3)  0           bn_data[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv0 (Conv2D)                  (None, 128, 128, 64) 9408        zero_padding2d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn0 (BatchNormalization)        (None, 128, 128, 64) 256         conv0[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "relu0 (Activation)              (None, 128, 128, 64) 0           bn0[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 130, 130, 64) 0           relu0[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "pooling0 (MaxPooling2D)         (None, 64, 64, 64)   0           zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_bn1 (BatchNormaliz (None, 64, 64, 64)   256         pooling0[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_relu1 (Activation) (None, 64, 64, 64)   0           stage1_unit1_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPadding2D (None, 66, 66, 64)   0           stage1_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_conv1 (Conv2D)     (None, 64, 64, 64)   36864       zero_padding2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_bn2 (BatchNormaliz (None, 64, 64, 64)   256         stage1_unit1_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_relu2 (Activation) (None, 64, 64, 64)   0           stage1_unit1_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPadding2D (None, 66, 66, 64)   0           stage1_unit1_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_conv2 (Conv2D)     (None, 64, 64, 64)   36864       zero_padding2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_sc (Conv2D)        (None, 64, 64, 64)   4096        stage1_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 64, 64, 64)   0           stage1_unit1_conv2[0][0]         \n",
      "                                                                 stage1_unit1_sc[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_bn1 (BatchNormaliz (None, 64, 64, 64)   256         add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_relu1 (Activation) (None, 64, 64, 64)   0           stage1_unit2_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPadding2D (None, 66, 66, 64)   0           stage1_unit2_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_conv1 (Conv2D)     (None, 64, 64, 64)   36864       zero_padding2d_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_bn2 (BatchNormaliz (None, 64, 64, 64)   256         stage1_unit2_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_relu2 (Activation) (None, 64, 64, 64)   0           stage1_unit2_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPadding2D (None, 66, 66, 64)   0           stage1_unit2_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_conv2 (Conv2D)     (None, 64, 64, 64)   36864       zero_padding2d_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 64, 64, 64)   0           stage1_unit2_conv2[0][0]         \n",
      "                                                                 add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_bn1 (BatchNormaliz (None, 64, 64, 64)   256         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_relu1 (Activation) (None, 64, 64, 64)   0           stage2_unit1_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_6 (ZeroPadding2D (None, 66, 66, 64)   0           stage2_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_conv1 (Conv2D)     (None, 32, 32, 128)  73728       zero_padding2d_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_bn2 (BatchNormaliz (None, 32, 32, 128)  512         stage2_unit1_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_relu2 (Activation) (None, 32, 32, 128)  0           stage2_unit1_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_7 (ZeroPadding2D (None, 34, 34, 128)  0           stage2_unit1_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_conv2 (Conv2D)     (None, 32, 32, 128)  147456      zero_padding2d_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_sc (Conv2D)        (None, 32, 32, 128)  8192        stage2_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 128)  0           stage2_unit1_conv2[0][0]         \n",
      "                                                                 stage2_unit1_sc[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_bn1 (BatchNormaliz (None, 32, 32, 128)  512         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_relu1 (Activation) (None, 32, 32, 128)  0           stage2_unit2_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_8 (ZeroPadding2D (None, 34, 34, 128)  0           stage2_unit2_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_conv1 (Conv2D)     (None, 32, 32, 128)  147456      zero_padding2d_8[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_bn2 (BatchNormaliz (None, 32, 32, 128)  512         stage2_unit2_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_relu2 (Activation) (None, 32, 32, 128)  0           stage2_unit2_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_9 (ZeroPadding2D (None, 34, 34, 128)  0           stage2_unit2_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_conv2 (Conv2D)     (None, 32, 32, 128)  147456      zero_padding2d_9[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 128)  0           stage2_unit2_conv2[0][0]         \n",
      "                                                                 add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_bn1 (BatchNormaliz (None, 32, 32, 128)  512         add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_relu1 (Activation) (None, 32, 32, 128)  0           stage3_unit1_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_10 (ZeroPadding2 (None, 34, 34, 128)  0           stage3_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_conv1 (Conv2D)     (None, 16, 16, 256)  294912      zero_padding2d_10[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_bn2 (BatchNormaliz (None, 16, 16, 256)  1024        stage3_unit1_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_relu2 (Activation) (None, 16, 16, 256)  0           stage3_unit1_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_11 (ZeroPadding2 (None, 18, 18, 256)  0           stage3_unit1_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_conv2 (Conv2D)     (None, 16, 16, 256)  589824      zero_padding2d_11[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_sc (Conv2D)        (None, 16, 16, 256)  32768       stage3_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 16, 256)  0           stage3_unit1_conv2[0][0]         \n",
      "                                                                 stage3_unit1_sc[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_bn1 (BatchNormaliz (None, 16, 16, 256)  1024        add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_relu1 (Activation) (None, 16, 16, 256)  0           stage3_unit2_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_12 (ZeroPadding2 (None, 18, 18, 256)  0           stage3_unit2_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_conv1 (Conv2D)     (None, 16, 16, 256)  589824      zero_padding2d_12[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_bn2 (BatchNormaliz (None, 16, 16, 256)  1024        stage3_unit2_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_relu2 (Activation) (None, 16, 16, 256)  0           stage3_unit2_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_13 (ZeroPadding2 (None, 18, 18, 256)  0           stage3_unit2_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_conv2 (Conv2D)     (None, 16, 16, 256)  589824      zero_padding2d_13[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 16, 256)  0           stage3_unit2_conv2[0][0]         \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_bn1 (BatchNormaliz (None, 16, 16, 256)  1024        add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_relu1 (Activation) (None, 16, 16, 256)  0           stage4_unit1_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_14 (ZeroPadding2 (None, 18, 18, 256)  0           stage4_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_conv1 (Conv2D)     (None, 8, 8, 512)    1179648     zero_padding2d_14[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_bn2 (BatchNormaliz (None, 8, 8, 512)    2048        stage4_unit1_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_relu2 (Activation) (None, 8, 8, 512)    0           stage4_unit1_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_15 (ZeroPadding2 (None, 10, 10, 512)  0           stage4_unit1_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_conv2 (Conv2D)     (None, 8, 8, 512)    2359296     zero_padding2d_15[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_sc (Conv2D)        (None, 8, 8, 512)    131072      stage4_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 8, 8, 512)    0           stage4_unit1_conv2[0][0]         \n",
      "                                                                 stage4_unit1_sc[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_bn1 (BatchNormaliz (None, 8, 8, 512)    2048        add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_relu1 (Activation) (None, 8, 8, 512)    0           stage4_unit2_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_16 (ZeroPadding2 (None, 10, 10, 512)  0           stage4_unit2_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_conv1 (Conv2D)     (None, 8, 8, 512)    2359296     zero_padding2d_16[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_bn2 (BatchNormaliz (None, 8, 8, 512)    2048        stage4_unit2_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_relu2 (Activation) (None, 8, 8, 512)    0           stage4_unit2_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_17 (ZeroPadding2 (None, 10, 10, 512)  0           stage4_unit2_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_conv2 (Conv2D)     (None, 8, 8, 512)    2359296     zero_padding2d_17[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 8, 8, 512)    0           stage4_unit2_conv2[0][0]         \n",
      "                                                                 add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "bn1 (BatchNormalization)        (None, 8, 8, 512)    2048        add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "relu1 (Activation)              (None, 8, 8, 512)    0           bn1[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 512)          0           relu1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          262656      global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 512)          2048        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            513         batch_normalization[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 11,452,106\n",
      "Trainable params: 11,443,140\n",
      "Non-trainable params: 8,966\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "bNXelbMQtonr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4G5O4jd6TuAG"
   },
   "source": [
    "### Continue training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0foWUN-yDLo_"
   },
   "source": [
    "If you trained to convergence earlier, this step will improve your accuracy by a few percentage points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "ECQLkAsFTlun"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "107/107 [==============================] - 11s 100ms/step - loss: 95.4865 - mse: 9143.0830 - val_loss: 95.4037 - val_mse: 9121.6406 - lr: 1.0000e-04\n",
      "Epoch 2/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 94.9615 - mse: 9040.0586 - val_loss: 94.2143 - val_mse: 8895.8740 - lr: 1.0000e-04\n",
      "Epoch 3/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 94.1634 - mse: 8890.6768 - val_loss: 93.3055 - val_mse: 8725.4082 - lr: 1.0000e-04\n",
      "Epoch 4/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 93.0824 - mse: 8690.9844 - val_loss: 91.9988 - val_mse: 8484.9531 - lr: 1.0000e-04\n",
      "Epoch 5/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 91.7196 - mse: 8440.4170 - val_loss: 90.3722 - val_mse: 8187.7598 - lr: 1.0000e-04\n",
      "Epoch 6/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 90.0802 - mse: 8140.2310 - val_loss: 86.9427 - val_mse: 7588.7197 - lr: 1.0000e-04\n",
      "Epoch 7/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 88.1711 - mse: 7797.4131 - val_loss: 88.7577 - val_mse: 7896.5420 - lr: 1.0000e-04\n",
      "Epoch 8/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 85.9998 - mse: 7420.7227 - val_loss: 85.4903 - val_mse: 7325.9053 - lr: 1.0000e-04\n",
      "Epoch 9/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 83.5743 - mse: 7006.9897 - val_loss: 81.5261 - val_mse: 6669.3833 - lr: 1.0000e-04\n",
      "Epoch 10/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 80.9025 - mse: 6572.2383 - val_loss: 80.5813 - val_mse: 6577.5791 - lr: 1.0000e-04\n",
      "Epoch 11/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 77.9921 - mse: 6118.7461 - val_loss: 81.0071 - val_mse: 6617.0283 - lr: 1.0000e-04\n",
      "Epoch 12/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 74.8503 - mse: 5632.7979 - val_loss: 71.5194 - val_mse: 5142.3198 - lr: 1.0000e-04\n",
      "Epoch 13/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 71.4842 - mse: 5136.4375 - val_loss: 71.3357 - val_mse: 5139.4551 - lr: 1.0000e-04\n",
      "Epoch 14/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 67.9005 - mse: 4639.6284 - val_loss: 65.8625 - val_mse: 4362.5757 - lr: 1.0000e-04\n",
      "Epoch 15/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 64.1054 - mse: 4134.2046 - val_loss: 67.0088 - val_mse: 4520.9902 - lr: 1.0000e-04\n",
      "Epoch 16/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 60.1047 - mse: 3640.2185 - val_loss: 65.7940 - val_mse: 4400.9790 - lr: 1.0000e-04\n",
      "Epoch 17/200\n",
      "107/107 [==============================] - 10s 93ms/step - loss: 55.9040 - mse: 3161.9573 - val_loss: 64.4045 - val_mse: 4418.5200 - lr: 1.0000e-04\n",
      "Epoch 18/200\n",
      "107/107 [==============================] - ETA: 0s - loss: 51.5082 - mse: 2689.6853\n",
      "New peak val_mae reached:   46.1\n",
      "/hdd/data/radioterapia/ciolaplata/models/1612839308.resnet18.23456.h5\n",
      "waiting epochs reset.\n",
      "107/107 [==============================] - 11s 102ms/step - loss: 51.5082 - mse: 2689.6853 - val_loss: 46.0884 - val_mse: 2152.5676 - lr: 1.0000e-04\n",
      "Epoch 19/200\n",
      "107/107 [==============================] - ETA: 0s - loss: 46.9223 - mse: 2231.6228\n",
      "New peak val_mae reached:   44.1\n",
      "/hdd/data/radioterapia/ciolaplata/models/1612839319.resnet18.23456.h5\n",
      "waiting epochs reset.\n",
      "107/107 [==============================] - 11s 102ms/step - loss: 46.9223 - mse: 2231.6228 - val_loss: 44.1173 - val_mse: 1979.2244 - lr: 1.0000e-04\n",
      "Epoch 20/200\n",
      "107/107 [==============================] - ETA: 0s - loss: 42.1507 - mse: 1807.6226\n",
      "New peak val_mae reached:   37.2\n",
      "/hdd/data/radioterapia/ciolaplata/models/1612839330.resnet18.23456.h5\n",
      "waiting epochs reset.\n",
      "107/107 [==============================] - 11s 102ms/step - loss: 42.1507 - mse: 1807.6226 - val_loss: 37.2205 - val_mse: 1411.6213 - lr: 1.0000e-04\n",
      "Epoch 21/200\n",
      "107/107 [==============================] - ETA: 0s - loss: 37.1975 - mse: 1410.4758\n",
      "New peak val_mae reached:   32.0\n",
      "/hdd/data/radioterapia/ciolaplata/models/1612839342.resnet18.23456.h5\n",
      "waiting epochs reset.\n",
      "107/107 [==============================] - 11s 101ms/step - loss: 37.1975 - mse: 1410.4758 - val_loss: 32.0029 - val_mse: 1055.4396 - lr: 1.0000e-04\n",
      "Epoch 22/200\n",
      "107/107 [==============================] - ETA: 0s - loss: 32.0666 - mse: 1058.7349\n",
      "New peak val_mae reached:   30.5\n",
      "/hdd/data/radioterapia/ciolaplata/models/1612839353.resnet18.23456.h5\n",
      "waiting epochs reset.\n",
      "107/107 [==============================] - 11s 103ms/step - loss: 32.0666 - mse: 1058.7349 - val_loss: 30.5150 - val_mse: 970.6611 - lr: 1.0000e-04\n",
      "Epoch 23/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 26.8110 - mse: 788.8702 - val_loss: 3540.3521 - val_mse: 13992277.0000 - lr: 1.0000e-04\n",
      "Epoch 24/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 21.3493 - mse: 497.2616 - val_loss: 98.6027 - val_mse: 13164.4990 - lr: 1.0000e-04\n",
      "Epoch 25/200\n",
      "107/107 [==============================] - ETA: 0s - loss: 15.8582 - mse: 299.5833\n",
      "New peak val_mae reached:    8.4\n",
      "/hdd/data/radioterapia/ciolaplata/models/1612839384.resnet18.23456.h5\n",
      "waiting epochs reset.\n",
      "107/107 [==============================] - 11s 102ms/step - loss: 15.8582 - mse: 299.5833 - val_loss: 8.3978 - val_mse: 86.6336 - lr: 1.0000e-04\n",
      "Epoch 26/200\n",
      "107/107 [==============================] - ETA: 0s - loss: 10.2928 - mse: 139.5025\n",
      "New peak val_mae reached:    2.2\n",
      "/hdd/data/radioterapia/ciolaplata/models/1612839395.resnet18.23456.h5\n",
      "waiting epochs reset.\n",
      "107/107 [==============================] - 11s 101ms/step - loss: 10.2928 - mse: 139.5025 - val_loss: 2.1962 - val_mse: 13.1613 - lr: 1.0000e-04\n",
      "Epoch 27/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 5.1887 - mse: 45.3111 - val_loss: 8.9503 - val_mse: 137.6114 - lr: 1.0000e-04\n",
      "Epoch 28/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 2.6401 - mse: 15.0010 - val_loss: 23.4263 - val_mse: 1780.0371 - lr: 1.0000e-04\n",
      "Epoch 29/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 2.1874 - mse: 11.8110 - val_loss: 5.5509 - val_mse: 69.7660 - lr: 1.0000e-04\n",
      "Epoch 30/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 2.0732 - mse: 10.7072 - val_loss: 2.4605 - val_mse: 17.7250 - lr: 1.0000e-04\n",
      "Epoch 31/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 2.0351 - mse: 10.7142 - val_loss: 3.7124 - val_mse: 18.1419 - lr: 1.0000e-04\n",
      "Epoch 32/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 2.0336 - mse: 10.4765 - val_loss: 3.7251 - val_mse: 28.7997 - lr: 1.0000e-04\n",
      "Epoch 33/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 1.9762 - mse: 9.9977 - val_loss: 4.2652 - val_mse: 23.7115 - lr: 1.0000e-04\n",
      "Epoch 34/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 1.9571 - mse: 9.8463 - val_loss: 2.8320 - val_mse: 12.9604 - lr: 1.0000e-04\n",
      "Epoch 35/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 1.9548 - mse: 9.7667 - val_loss: 2.4426 - val_mse: 15.5727 - lr: 1.0000e-04\n",
      "Epoch 36/200\n",
      "107/107 [==============================] - ETA: 0s - loss: 1.9032 - mse: 9.1699\n",
      "New peak val_mae reached:   2.04\n",
      "/hdd/data/radioterapia/ciolaplata/models/1612839498.resnet18.23456.h5\n",
      "waiting epochs reset.\n",
      "107/107 [==============================] - 11s 102ms/step - loss: 1.9032 - mse: 9.1699 - val_loss: 2.0372 - val_mse: 10.6354 - lr: 1.0000e-04\n",
      "Epoch 37/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 1.9025 - mse: 9.4742 - val_loss: 2.7685 - val_mse: 12.5503 - lr: 1.0000e-04\n",
      "Epoch 38/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 1.8908 - mse: 8.9520 - val_loss: 3.1536 - val_mse: 19.7473 - lr: 1.0000e-04\n",
      "Epoch 39/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 1.9186 - mse: 9.5695 - val_loss: 3.3243 - val_mse: 18.9327 - lr: 1.0000e-04\n",
      "Epoch 40/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 1.8146 - mse: 8.6087 - val_loss: 2.1105 - val_mse: 9.4478 - lr: 1.0000e-04\n",
      "Epoch 41/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 1.8262 - mse: 8.6644 - val_loss: 2.5903 - val_mse: 13.5002 - lr: 1.0000e-04\n",
      "Epoch 42/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107/107 [==============================] - 10s 92ms/step - loss: 1.8580 - mse: 9.0455 - val_loss: 5.7883 - val_mse: 49.3190 - lr: 1.0000e-04\n",
      "Epoch 43/200\n",
      "107/107 [==============================] - ETA: 0s - loss: 1.7950 - mse: 8.3591\n",
      "New peak val_mae reached:   1.81\n",
      "/hdd/data/radioterapia/ciolaplata/models/1612839570.resnet18.23456.h5\n",
      "waiting epochs reset.\n",
      "107/107 [==============================] - 11s 103ms/step - loss: 1.7950 - mse: 8.3591 - val_loss: 1.8117 - val_mse: 8.4744 - lr: 1.0000e-04\n",
      "Epoch 44/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 1.8310 - mse: 8.8572 - val_loss: 2.0420 - val_mse: 8.3529 - lr: 1.0000e-04\n",
      "Epoch 45/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 1.7344 - mse: 7.8408 - val_loss: 2.3760 - val_mse: 13.5461 - lr: 1.0000e-04\n",
      "Epoch 46/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 1.7254 - mse: 7.7140 - val_loss: 5.7135 - val_mse: 41.2738 - lr: 1.0000e-04\n",
      "Epoch 47/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 1.7783 - mse: 8.3875 - val_loss: 2.2645 - val_mse: 9.6938 - lr: 1.0000e-04\n",
      "Epoch 48/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 1.6868 - mse: 7.5090 - val_loss: 2.0770 - val_mse: 9.0986 - lr: 1.0000e-04\n",
      "Epoch 49/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 1.6360 - mse: 7.0409 - val_loss: 2.2763 - val_mse: 9.2299 - lr: 1.0000e-04\n",
      "Epoch 50/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 1.6317 - mse: 7.0998 - val_loss: 2.3537 - val_mse: 10.0378 - lr: 1.0000e-04\n",
      "Epoch 51/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 1.6032 - mse: 6.8523 - val_loss: 2.2972 - val_mse: 13.3469 - lr: 1.0000e-04\n",
      "Epoch 52/200\n",
      "107/107 [==============================] - 10s 93ms/step - loss: 1.5259 - mse: 6.3257 - val_loss: 3.5845 - val_mse: 19.0157 - lr: 1.0000e-04\n",
      "Epoch 53/200\n",
      "107/107 [==============================] - 10s 93ms/step - loss: 1.5298 - mse: 5.9872 - val_loss: 2.1824 - val_mse: 13.8247 - lr: 1.0000e-04\n",
      "Epoch 54/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 1.4590 - mse: 5.5917 - val_loss: 3.9370 - val_mse: 27.2939 - lr: 1.0000e-04\n",
      "Epoch 55/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 1.4544 - mse: 5.5873 - val_loss: 2.1725 - val_mse: 9.8130 - lr: 1.0000e-04\n",
      "Epoch 56/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 1.4281 - mse: 5.4111 - val_loss: 2.0446 - val_mse: 10.2692 - lr: 1.0000e-04\n",
      "Epoch 57/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 1.3805 - mse: 4.9468 - val_loss: 1.8624 - val_mse: 8.6316 - lr: 1.0000e-04\n",
      "Epoch 58/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 1.3535 - mse: 4.8497 - val_loss: 3.0553 - val_mse: 16.7386 - lr: 1.0000e-04\n",
      "Epoch 59/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 1.2937 - mse: 4.3985 - val_loss: 3.5910 - val_mse: 17.6501 - lr: 1.0000e-04\n",
      "Epoch 60/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 1.2717 - mse: 4.1369 - val_loss: 2.0200 - val_mse: 9.0904 - lr: 1.0000e-04\n",
      "Epoch 61/200\n",
      "107/107 [==============================] - 10s 93ms/step - loss: 1.2583 - mse: 4.0059 - val_loss: 2.1662 - val_mse: 12.6836 - lr: 1.0000e-04\n",
      "Epoch 62/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 1.2098 - mse: 3.5775 - val_loss: 2.2148 - val_mse: 11.8379 - lr: 1.0000e-04\n",
      "Epoch 63/200\n",
      "107/107 [==============================] - ETA: 0s - loss: 1.2053 - mse: 3.6031\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 1.2053 - mse: 3.6031 - val_loss: 2.2054 - val_mse: 9.4199 - lr: 1.0000e-04\n",
      "Epoch 64/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 1.0591 - mse: 2.8788 - val_loss: 2.5014 - val_mse: 12.7820 - lr: 2.0000e-05\n",
      "Epoch 65/200\n",
      "107/107 [==============================] - 10s 93ms/step - loss: 0.9580 - mse: 2.2937 - val_loss: 1.8826 - val_mse: 7.7058 - lr: 2.0000e-05\n",
      "Epoch 66/200\n",
      "107/107 [==============================] - ETA: 0s - loss: 1.0228 - mse: 2.6455\n",
      "New peak val_mae reached:   1.71\n",
      "/hdd/data/radioterapia/ciolaplata/models/1612839805.resnet18.23456.h5\n",
      "waiting epochs reset.\n",
      "107/107 [==============================] - 11s 102ms/step - loss: 1.0228 - mse: 2.6455 - val_loss: 1.7104 - val_mse: 7.6164 - lr: 2.0000e-05\n",
      "Epoch 67/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 0.9665 - mse: 2.3305 - val_loss: 1.7725 - val_mse: 8.2851 - lr: 2.0000e-05\n",
      "Epoch 68/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 0.9254 - mse: 2.1467 - val_loss: 1.9626 - val_mse: 8.9418 - lr: 2.0000e-05\n",
      "Epoch 69/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 0.9421 - mse: 2.1424 - val_loss: 1.9546 - val_mse: 8.5398 - lr: 2.0000e-05\n",
      "Epoch 70/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 0.9000 - mse: 2.0989 - val_loss: 2.0628 - val_mse: 8.0754 - lr: 2.0000e-05\n",
      "Epoch 71/200\n",
      "107/107 [==============================] - 10s 93ms/step - loss: 0.8961 - mse: 1.9971 - val_loss: 1.7040 - val_mse: 7.4459 - lr: 2.0000e-05\n",
      "Epoch 72/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 0.8621 - mse: 1.9247 - val_loss: 1.7424 - val_mse: 7.4581 - lr: 2.0000e-05\n",
      "Epoch 73/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 0.8276 - mse: 1.7678 - val_loss: 1.7320 - val_mse: 7.9727 - lr: 2.0000e-05\n",
      "Epoch 74/200\n",
      "107/107 [==============================] - 10s 93ms/step - loss: 0.9490 - mse: 2.1631 - val_loss: 1.7854 - val_mse: 7.8344 - lr: 2.0000e-05\n",
      "Epoch 75/200\n",
      "107/107 [==============================] - 10s 93ms/step - loss: 0.8898 - mse: 1.9701 - val_loss: 1.7160 - val_mse: 7.5292 - lr: 2.0000e-05\n",
      "Epoch 76/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 0.8653 - mse: 1.8543 - val_loss: 1.8218 - val_mse: 7.5719 - lr: 2.0000e-05\n",
      "Epoch 77/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 0.8033 - mse: 1.7320 - val_loss: 1.7079 - val_mse: 7.5895 - lr: 2.0000e-05\n",
      "Epoch 78/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 0.8527 - mse: 1.9031 - val_loss: 1.9359 - val_mse: 7.7757 - lr: 2.0000e-05\n",
      "Epoch 79/200\n",
      "107/107 [==============================] - 10s 93ms/step - loss: 0.8637 - mse: 1.8777 - val_loss: 1.7216 - val_mse: 7.6084 - lr: 2.0000e-05\n",
      "Epoch 80/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 0.9010 - mse: 2.0199 - val_loss: 1.7425 - val_mse: 7.4551 - lr: 2.0000e-05\n",
      "Epoch 81/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 0.8259 - mse: 1.7008 - val_loss: 1.8968 - val_mse: 8.9224 - lr: 2.0000e-05\n",
      "Epoch 82/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 0.8511 - mse: 1.7179 - val_loss: 1.7118 - val_mse: 7.4734 - lr: 2.0000e-05\n",
      "Epoch 83/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 0.8403 - mse: 1.7707 - val_loss: 2.1398 - val_mse: 8.4496 - lr: 2.0000e-05\n",
      "Epoch 84/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 0.8220 - mse: 1.6035 - val_loss: 2.3352 - val_mse: 9.7749 - lr: 2.0000e-05\n",
      "Epoch 85/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 0.8249 - mse: 1.6251 - val_loss: 1.9013 - val_mse: 7.6032 - lr: 2.0000e-05\n",
      "Epoch 86/200\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.6965 - mse: 1.2268\n",
      "Epoch 00086: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 0.6965 - mse: 1.2268 - val_loss: 1.9439 - val_mse: 8.2725 - lr: 2.0000e-05\n",
      "Epoch 87/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 0.8015 - mse: 1.5759 - val_loss: 1.7215 - val_mse: 7.6276 - lr: 4.0000e-06\n",
      "Epoch 88/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 0.8019 - mse: 1.6342 - val_loss: 1.7181 - val_mse: 7.5092 - lr: 4.0000e-06\n",
      "Epoch 89/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 0.7349 - mse: 1.3721 - val_loss: 1.7160 - val_mse: 7.4026 - lr: 4.0000e-06\n",
      "Epoch 90/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 0.7675 - mse: 1.4107 - val_loss: 1.7137 - val_mse: 7.3897 - lr: 4.0000e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 0.6532 - mse: 1.0467 - val_loss: 1.7348 - val_mse: 7.6162 - lr: 4.0000e-06\n",
      "Epoch 92/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 0.8158 - mse: 1.6635 - val_loss: 1.7190 - val_mse: 7.4428 - lr: 4.0000e-06\n",
      "Epoch 93/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 0.8350 - mse: 1.7233 - val_loss: 1.7390 - val_mse: 7.6421 - lr: 4.0000e-06\n",
      "Epoch 94/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 0.7656 - mse: 1.4884 - val_loss: 1.7253 - val_mse: 7.5511 - lr: 4.0000e-06\n",
      "Epoch 95/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 0.8447 - mse: 1.8221 - val_loss: 1.7172 - val_mse: 7.3888 - lr: 4.0000e-06\n",
      "Epoch 96/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 0.8141 - mse: 1.6270 - val_loss: 1.7287 - val_mse: 7.4155 - lr: 4.0000e-06\n",
      "Epoch 97/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 0.7978 - mse: 1.5744 - val_loss: 1.7411 - val_mse: 7.3686 - lr: 4.0000e-06\n",
      "Epoch 98/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 0.6713 - mse: 1.1641 - val_loss: 1.7126 - val_mse: 7.3528 - lr: 4.0000e-06\n",
      "Epoch 99/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 0.7652 - mse: 1.4784 - val_loss: 1.7159 - val_mse: 7.3677 - lr: 4.0000e-06\n",
      "Epoch 100/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 0.8006 - mse: 1.5263 - val_loss: 1.7910 - val_mse: 7.3810 - lr: 4.0000e-06\n",
      "Epoch 101/200\n",
      "107/107 [==============================] - 10s 93ms/step - loss: 0.7324 - mse: 1.3417 - val_loss: 1.7466 - val_mse: 7.6090 - lr: 4.0000e-06\n",
      "Epoch 102/200\n",
      "107/107 [==============================] - 10s 93ms/step - loss: 0.7967 - mse: 1.6272 - val_loss: 1.7174 - val_mse: 7.4197 - lr: 4.0000e-06\n",
      "Epoch 103/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 0.7484 - mse: 1.4000 - val_loss: 1.7282 - val_mse: 7.2841 - lr: 4.0000e-06\n",
      "Epoch 104/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 0.7777 - mse: 1.4217 - val_loss: 1.7247 - val_mse: 7.3340 - lr: 4.0000e-06\n",
      "Epoch 105/200\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 0.7720 - mse: 1.4195 - val_loss: 1.7293 - val_mse: 7.5260 - lr: 4.0000e-06\n",
      "Epoch 106/200\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.7606 - mse: 1.4059\n",
      "Epoch 00106: ReduceLROnPlateau reducing learning rate to 7.999999979801942e-07.\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 0.7606 - mse: 1.4059 - val_loss: 1.7105 - val_mse: 7.3978 - lr: 4.0000e-06\n",
      "Epoch 107/200\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.7931 - mse: 1.5129\n",
      "Stopping early at epoch 107 with saved peak mae  1.7103997\n",
      "107/107 [==============================] - 10s 92ms/step - loss: 0.7931 - mse: 1.5129 - val_loss: 1.7123 - val_mse: 7.3196 - lr: 8.0000e-07\n"
     ]
    }
   ],
   "source": [
    "fine_tune_epochs = ARG_MAX_FINE_TUNING_EPOCHS\n",
    "try:\n",
    "    history.epoch\n",
    "except NameError:\n",
    "    total_epochs = fine_tune_epochs\n",
    "    initial_epoch =  0\n",
    "else:\n",
    "    total_epochs =  (history.epoch[-1]+1) + fine_tune_epochs\n",
    "    initial_epoch =  history.epoch[-1]+1\n",
    "\n",
    "# by default we use custom LR schedule\n",
    "callbacks=[callbackObj, callbackLR]\n",
    "if ARG_LR_SCHEDULE == 1:\n",
    "    callbacks=[callbackObj, reduce_lr]\n",
    "    \n",
    "history_fine = model.fit(tmp_train_batches,\n",
    "                         epochs=total_epochs,\n",
    "                         initial_epoch =  initial_epoch,\n",
    "                         validation_data=tmp_validation_batches,\n",
    "                         callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "vjiNEG47yFVY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106]\n"
     ]
    }
   ],
   "source": [
    "print(history_fine.epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_fine.history['val_mse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PpA8PlpQKygw"
   },
   "outputs": [],
   "source": [
    "mse += history_fine.history['mse']\n",
    "val_mse += history_fine.history['val_mse']\n",
    "\n",
    "loss += history_fine.history['loss']\n",
    "val_loss += history_fine.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Igm8aMrLyFVj"
   },
   "outputs": [],
   "source": [
    "mse = mse[1:]\n",
    "val_mse = val_mse[1:]\n",
    "loss = loss[1:]\n",
    "val_loss = val_loss[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "chW103JUItdk"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(mse, label='Training MSE')\n",
    "plt.plot(val_mse, label='Validation MSE')\n",
    "plt.ylim([0, 30])\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "          plt.ylim(), label='Fine Tuning starts')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Training and Validation MSE')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.ylim([0, 5])\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "         plt.ylim(), label='Fine Tuning Starts')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TZTwG7nhm0C"
   },
   "source": [
    "## Summary:\n",
    "\n",
    "* **Using a pre-trained model for feature extraction**:  When working with a small dataset, it is a common practice to take advantage of features learned by a model trained on a larger dataset in the same domain. This is done by instantiating the pre-trained model and adding a fully-connected classifier on top. The pre-trained model is \"frozen\" and only the weights of the classifier get updated during training.\n",
    "In this case, the convolutional base extracted all the features associated with each image and you just trained a classifier that determines the image class given that set of extracted features.\n",
    "\n",
    "* **Fine-tuning a pre-trained model**: To further improve performance, one might want to repurpose the top-level layers of the pre-trained models to the new dataset via fine-tuning.\n",
    "In this case, you tuned your weights such that your model learned high-level features specific to the dataset. This technique is usually recommended when the training dataset is large and very similar to the original dataset that the pre-trained model was trained on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eQy9mVkuoFWj"
   },
   "outputs": [],
   "source": [
    "if isGoogleColab():\n",
    "    !ls -l \"/content/drive/MyDrive/Healthcare/Radioterapia/data/ciolaplata/models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "f4tr0VXGyFVr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best saved model file = /hdd/data/radioterapia/ciolaplata/models/1612839805.resnet18.23456.h5\n"
     ]
    }
   ],
   "source": [
    "print(f'Best saved model file = {callbackObj.saved_model_file}')\n",
    "saved_model = callbackObj.saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "esaw0xjhyFVv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107/107 - 3s - loss: 0.5827 - mean_squared_error: 0.9644\n",
      "Saved model, train loss: 0.5827\n",
      "Saved model, train mse: 0.9644\n",
      "\n",
      "27/27 - 1s - loss: 1.7104 - mean_squared_error: 7.6164\n",
      "Saved model, validation loss: 1.7104\n",
      "Saved model, validation mse: 7.6164\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the saved model on the train set which differs from values reported during training\n",
    "loss, mse = saved_model.evaluate(tmp_train_batches, verbose=2)\n",
    "print(\"Saved model, train loss: {:5.4f}\".format(loss))\n",
    "print('Saved model, train mse: {:5.4f}\\n'.format(mse))\n",
    "\n",
    "# Evaluate the saved model on the validation set which differs from values reported during training\n",
    "loss, mse = saved_model.evaluate(tmp_validation_batches, verbose=2)\n",
    "print(\"Saved model, validation loss: {:5.4f}\".format(loss))\n",
    "print('Saved model, validation mse: {:5.4f}'.format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qACCSCEAyFVz"
   },
   "outputs": [],
   "source": [
    "#result_batch = model.predict(tmp_train_batches)\n",
    "#reloaded_result_batch = reloaded_model.predict(tmp_train_batches)\n",
    "#print(abs(reloaded_result_batch - result_batch).max())\n",
    "#np.testing.assert_allclose(result_batch, reloaded_result_batch, rtol=1e-6, atol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kcEpqDWFyFV4"
   },
   "outputs": [],
   "source": [
    "# projects out just the first two components.\n",
    "if ARG_TEST_PARTITION:\n",
    "    tmp_test_batches = test_batches.map(lambda image, gamma, filename: (image, gamma))\n",
    "    print(tmp_test_batches)\n",
    "\n",
    "    # Evaluate the reloaded model on the test set\n",
    "    loss, mse = saved_model.evaluate(tmp_test_batches, verbose=1)\n",
    "    print(\"\\n\\nSaved model, test loss: {:5.4f}\".format(loss))\n",
    "    print('Saved model, test mse: {:5.4f}'.format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R1oRiBayRPVt"
   },
   "outputs": [],
   "source": [
    "if isGoogleColab():\n",
    "    drive.flush_and_unmount()\n",
    "    print('All changes made in this colab session should now be visible in Drive.')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "transfer_learning.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
