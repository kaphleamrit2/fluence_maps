{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/pablojrios/fluence_maps/blob/master/tf2_transfer_learning_gamma.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRTa3Ee15WsJ"
   },
   "source": [
    "# Transfer learning with a pretrained ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q50x39yF5BPt"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "%cd '/content'\n",
    "if os.path.exists('fluence_maps'):\n",
    "  !rm -fr fluence_maps\n",
    "\n",
    "## Install required dependencies\n",
    "!pip install -q pydicom\n",
    "## to support ResNet18 and ResNet34\n",
    "!pip install image-classifiers\n",
    "## https://github.com/tensorflow/addons/issues/2251\n",
    "!pip install -U tensorflow-addons\n",
    "\n",
    "GIT_USERNAME = \"pablojrios\"\n",
    "GIT_TOKEN = \"1d88a0b85d2b00a03796e4d8b7e5f7b249b12f9b\"\n",
    "!git clone -s https://{GIT_TOKEN}@github.com/{GIT_USERNAME}/fluence_maps.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iBMcobPHdD8O"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import time\n",
    "from random import shuffle, randrange\n",
    "import random\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TqOt6Sv7AsMi"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print('Tensorflow version = {}, addons version = {}'.format(tf.__version__, tfa.__version__))\n",
    "print('Executing eagerly = {}'.format(tf.executing_eagerly()))\n",
    "keras = tf.keras\n",
    "\n",
    "# To support ResNet18 and ResNet34 for tensorflow.keras\n",
    "from classification_models.tfkeras import Classifiers\n",
    "\n",
    "import cv2 # to perform data augmentation\n",
    "print('OpenCV version = {}'.format(cv2.__version__))\n",
    "\n",
    "datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bs3exgGS09nQ"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uFuIiQp2zVUF"
   },
   "outputs": [],
   "source": [
    "# ===============================================DEFINE YOUR ARGUMENTS=================================================\n",
    "ARG_DATASET_DIR='/content/drive/My Drive/Healthcare/Radioterapia/data/ciolaplata'\n",
    "# folder under ARG_DATASET_DIR path.\n",
    "ARG_TFDATASET_FOLDER='tfds.2019.localnorm.ovs95'\n",
    "ARG_RANDOM_SEED = 23456\n",
    "# if False only training and validation partition are created.\n",
    "ARG_TEST_PARTITION=False\n",
    "# number of continuous epochs without improvement on validation\n",
    "ARG_EPOCHS_WO_IMPROVEMENT=30\n",
    "# maximum fine-tuning epochs\n",
    "ARG_MAX_FINE_TUNING_EPOCHS=200\n",
    "ARG_RESNET_NETWORK=True\n",
    "ARG_DATA_AUGMENTATION=True\n",
    "# perform data augmentation of images with a gamma value lower than gamma_augment\n",
    "ARG_GAMMA_AUGMENT=95.0\n",
    "# set this value based on the ARG_OVERSAMPLING_FACTOR value in tf2_oversampling_dicom_files.py\n",
    "# 1.0 means transform each and every image, with a lower value than 1.0 means that training will include unmodified (not transformed)\n",
    "# images.\n",
    "ARG_AUGMENT_PROBABILITY=0.75\n",
    " allows to train on ex)\n",
    "add_regularizers=False\n",
    "## Fine-tune from this layer onwards\n",
    "# fine_tune_at = 281 # InceptionV3, fine-tuning\n",
    "# fine_tune_at = 102 # InceptionV3, not so fine-tuning\n",
    "# fine_tune_at = 12 # VGG16\n",
    "fine_tune_at = 47 # resnet18 stage3\n",
    "# fine_tune_at = 74 # resnet34 stage3\n",
    "# fine_tune_at = 129 # resnet34 stage4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v77rlkCKW0IJ"
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0GoKGm1duzgk"
   },
   "source": [
    "### Data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KVh7rDVAuW8Y"
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "tf.random.set_seed(ARG_RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1KR9xb8NyFTC"
   },
   "outputs": [],
   "source": [
    "def _tfrecord_dataset_type_from_folder(folder, dataset_type, ext='.tfrecords'):\n",
    "    tfrecords = [os.path.join(folder, n)\n",
    "                 for n in os.listdir(folder) if n.startswith(dataset_type) and n.endswith(ext)]\n",
    "    return tf.data.TFRecordDataset(tfrecords)\n",
    "\n",
    "tfdataset_dir = os.path.join(ARG_DATASET_DIR, ARG_TFDATASET_FOLDER)\n",
    "raw_train = _tfrecord_dataset_type_from_folder(tfdataset_dir, 'train')\n",
    "raw_validation = _tfrecord_dataset_type_from_folder(tfdataset_dir, 'validation')\n",
    "if ARG_TEST_PARTITION:\n",
    "    raw_test = _tfrecord_dataset_type_from_folder(tfdataset_dir, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o29EfE-p0g5X"
   },
   "source": [
    "The resulting `tf.data.Dataset` objects contain `(image, label)` pairs where the images have variable shape and 3 channels, and the label is a scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GIys1_zY1S9b"
   },
   "outputs": [],
   "source": [
    "print(raw_train)\n",
    "print(raw_validation)\n",
    "if ARG_TEST_PARTITION:\n",
    "    print(raw_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T1Gm1wdHyFTK"
   },
   "outputs": [],
   "source": [
    "print(f'Number of images in validation part = {sum(1 for _ in raw_validation)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPyydf8h-ayL"
   },
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RL6NwT3O-kXL"
   },
   "source": [
    "### Center Crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AKkkw09v-MiA"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def crop(im, r, c, target_r, target_c): return im[r:r+target_r, c:c+target_c]\n",
    "\n",
    "# random crop to the original size\n",
    "def random_crop(x, r_pix=8):\n",
    "    \"\"\" Returns a random crop\"\"\"\n",
    "    r, c,*_ = x.shape\n",
    "    c_pix = round(r_pix*c/r)\n",
    "    rand_r = random.uniform(0, 1)\n",
    "    rand_c = random.uniform(0, 1)\n",
    "    start_r = np.floor(2*rand_r*r_pix).astype(int)\n",
    "    start_c = np.floor(2*rand_c*c_pix).astype(int)\n",
    "    return crop(x, start_r, start_c, r-2*r_pix, c-2*c_pix)\n",
    "\n",
    "def center_crop(x, r_pix=8):\n",
    "    r, c,*_ = x.shape\n",
    "    c_pix = round(r_pix*c/r)\n",
    "    return crop(x, r_pix, c_pix, r-2*r_pix, c-2*c_pix)\n",
    "\n",
    "\n",
    "def rotate_cv(im, deg, mode=cv2.BORDER_REFLECT, interpolation=cv2.INTER_AREA):\n",
    "    \"\"\" Rotates an image by deg degrees\"\"\"\n",
    "    r,c,*_ = im.shape\n",
    "    print(f\"rotate_cv: {r},{c}\")\n",
    "    M = cv2.getRotationMatrix2D((c/2,r/2),deg,1)\n",
    "    return cv2.warpAffine(im,M,(c,r), borderMode=mode, \n",
    "                          flags=cv2.WARP_FILL_OUTLIERS+interpolation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d56BNppM-tOp"
   },
   "source": [
    "### Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lOmtnOTa-1Sy"
   },
   "outputs": [],
   "source": [
    "def random_translation(x, t_pix=24):\n",
    "    \"\"\" Returns a random translation\"\"\"\n",
    "    rows, cols, *_ = x.shape\n",
    "    rand_r = random.uniform(0, 1)\n",
    "    rand_c = random.uniform(0, 1)\n",
    "    t_r = np.floor(rand_r*t_pix).astype(int)\n",
    "    t_c = np.floor(rand_c*t_pix).astype(int)\n",
    "    # transformation T does shift (t_r, t_c)\n",
    "    # [a0, a1, a2, b0, b1, b2, c0, c1], then it maps the output point\n",
    "    # (x, y) to a transformed input point\n",
    "    # (x', y') = ((a0 x + a1 y + a2) / k, (b0 x + b1 y + b2) / k),\n",
    "    # where k = c0 x + c1 y + 1\n",
    "    T = [1, 0, t_r, 0, 1, t_c, 0, 0]\n",
    "    return tfa.image.transform (x, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jXyYU618yFTP"
   },
   "outputs": [],
   "source": [
    "def rescale_0_to_1(image):\n",
    "    \"\"\"\n",
    "    Rescale image to [0, 1].\n",
    "    \"\"\"\n",
    "    # Image must be casted to float32 first\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = (image/255.0)\n",
    "    return image\n",
    "\n",
    "\n",
    "def rescale_min_1_to_1(image):\n",
    "    \"\"\"\n",
    "    Rescale image to [-1, 1].\n",
    "\n",
    "    :param image:\n",
    "        Required. Image tensor.\n",
    "\n",
    "    :return:\n",
    "        Scaled image.\n",
    "    \"\"\"\n",
    "    # Image must be casted to float32 first\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    # Rescale image from [0, 255] to [0, 2], and by substracting -1 we rescale to [-1, 1].\n",
    "    image = (image/127.5) - 1\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ar8TNzTSyFTS"
   },
   "outputs": [],
   "source": [
    "def _parse_image_function(example_proto, img_size, normalization_fn, data_augmentation=False, augment_probability=1.0):\n",
    "    # Create a dictionary describing the features.\n",
    "    image_feature_description = {\"image/filename\": tf.io.FixedLenFeature((), tf.string),\n",
    "                \"image/encoded\": tf.io.FixedLenFeature((), tf.string),\n",
    "                \"image/format\": tf.io.FixedLenFeature((), tf.string),\n",
    "                \"image/gamma_index\": tf.io.FixedLenFeature((), tf.float32),\n",
    "                \"image/height\": tf.io.FixedLenFeature((), tf.int64),\n",
    "                \"image/width\": tf.io.FixedLenFeature((), tf.int64)}\n",
    "    \n",
    "    def image_augment(image):\n",
    "        radian = ((np.random.random()-.50)*20 / 360) * np.pi\n",
    "        # tfa.image.transform_ops.rotate(image, radian) is an alias\n",
    "        image = tfa.image.rotate(image, radian)\n",
    "        # image = tf.image.random_flip_left_right(image)\n",
    "        image = random_translation(image)\n",
    "        return image\n",
    "\n",
    "    # Now, globally set everything to run eagerly\n",
    "    # The following doesn't set to eager mode:\n",
    "    # UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set,\n",
    "    # this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
    "    # tf.config.run_functions_eagerly(True)\n",
    "\n",
    "    # Executing eagerly = False here!\n",
    "    # numpy is only supported in eager mode. If you are in graph mode, it will not be supported.\n",
    "    # In eager execution the shape is always fully-known.\n",
    "    # print('Executing eagerly = {}'.format(tf.executing_eagerly()))\n",
    "\n",
    "    # Parse the input tf.Example proto using the dictionary above.\n",
    "    parsed = tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "    \n",
    "    image = tf.image.decode_jpeg(parsed[\"image/encoded\"], channels=3)\n",
    "    # print(type(image), image.shape, image.dtype) # <class 'tensorflow.python.framework.ops.Tensor'> (None, None, 3) <dtype: 'uint8'>\n",
    "\n",
    "    gamma = tf.cast(\n",
    "        parsed[\"image/gamma_index\"],\n",
    "        tf.float32)\n",
    "    # print(type(gamma), gamma.shape, gamma.dtype) # <class 'tensorflow.python.framework.ops.Tensor'> () <dtype: 'float32'>\n",
    "\n",
    "    image = normalization_fn(image)\n",
    "\n",
    "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    print(type(image), image.shape, image.dtype)\n",
    "\n",
    "    if data_augmentation and augment_probability >= random.uniform(0, 1):\n",
    "        gamma_augment = tf.constant(ARG_GAMMA_AUGMENT)\n",
    "        image = tf.cond(tf.math.less(gamma, gamma_augment)\n",
    "                        , lambda: image_augment(image)\n",
    "                        , lambda: image)\n",
    "\n",
    "    # normalizo antes de transformar\n",
    "    # image = normalization_fn(image)\n",
    "    \n",
    "    #label is a tensor of an array of single tf.int64 arrays.\n",
    "    #label = tf.cast(\n",
    "    #    tf.reshape(parsed[\"image/class/label\"], [-1]),\n",
    "    #    tf.int64)\n",
    "\n",
    "    # assert tf.executing_eagerly() FAILS\n",
    "    # parsed[\"image/filename\"] is a Tensor and not an EagerTensor because we are in a map function,\n",
    "    # because in 2.0, code inside Datasets maps is turned into a subgraph for speed, just as it was in 1.x eager\n",
    "    # execution. You generally want to avoid Python inside your data pipeline.\n",
    "    # So, if I invoke parsed[\"image/filename\"].numpy().decode('utf-8') to get the filename string the error\n",
    "    # \"AttributeError: 'Tensor' object has no attribute 'numpy'\" is thrown, hence I return a tensor.\n",
    "    filename = parsed[\"image/filename\"]\n",
    "            \n",
    "    return image, gamma, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CyTNIMaeyFTX"
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 256 # All images will be resized to 256x256\n",
    "normalization_fn = rescale_0_to_1 # rescale_min_1_to_1\n",
    "# normalization_fn = tf.image.per_image_standardization # loss y mae en validación reportan números muy grandes,\n",
    "# no así en training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SFZ6ZW7KSXP9"
   },
   "outputs": [],
   "source": [
    "num_workers = 8\n",
    "\n",
    "# assert tf.executing_eagerly()\n",
    "if ARG_DATA_AUGMENTATION:\n",
    "    print(\"Training with image augmentation.\")\n",
    "    \n",
    "train = raw_train.map(lambda e: _parse_image_function(e, IMG_SIZE, normalization_fn, ARG_DATA_AUGMENTATION, ARG_AUGMENT_PROBABILITY),\n",
    "                      num_parallel_calls=num_workers)\n",
    "validation = raw_validation.map(lambda e: _parse_image_function(e, IMG_SIZE, normalization_fn),\n",
    "                                num_parallel_calls=num_workers)\n",
    "if ARG_TEST_PARTITION:\n",
    "    test = raw_test.map(lambda e: _parse_image_function(e, IMG_SIZE, normalization_fn),\n",
    "                        num_parallel_calls=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yic-I66m6Isv"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER_SIZE = 1000\n",
    "PREFETCH_BUFFER_SIZE = 2 * BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p3UUPdm86LNC"
   },
   "outputs": [],
   "source": [
    "print('Executing eagerly = {}'.format(tf.executing_eagerly()))\n",
    "\n",
    "# <PrefetchDataset shapes: ((None, 224, 224, 3), (None, 1)), types: (tf.float32, tf.int64)>\n",
    "train_batches = train.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE).prefetch(PREFETCH_BUFFER_SIZE)\n",
    "print(train_batches)\n",
    "validation_batches = validation.batch(BATCH_SIZE).prefetch(PREFETCH_BUFFER_SIZE)\n",
    "print(validation_batches)\n",
    "# <BatchDataset shapes: ((None, 224, 224, 3), (None, 1)), types: (tf.float32, tf.int64)>\n",
    "if ARG_TEST_PARTITION:\n",
    "    test_batches = test.batch(BATCH_SIZE)\n",
    "    print(test_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02rJpcFtChP0"
   },
   "source": [
    "Inspect a batch of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iknFo3ELBVho"
   },
   "outputs": [],
   "source": [
    "for image_batch, label_batch, filename_batch in train_batches.take(1):\n",
    "    pass\n",
    "\n",
    "image_batch.shape, label_batch.shape, filename_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5pIy5ehM4YzC"
   },
   "outputs": [],
   "source": [
    "# display 2nd image in the batch\n",
    "gamma_value = label_batch[7].numpy()\n",
    "filename = filename_batch[7].numpy().decode('utf-8')\n",
    "print(f'gamma={gamma_value:2.2f}, filename={filename}')\n",
    "# if pixel values are float they have to be in [0, 1] range, if they are integer they have to be in the [0, 255] range,\n",
    "# else pixel values are truncated.\n",
    "im = image_batch[7].numpy()\n",
    "plt.imshow(im)\n",
    "print(\"image shape = {}\".format(im.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OkH-kazQecHB"
   },
   "source": [
    "## Create the base model from the pre-trained convnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aiLcQ7JkDM6U"
   },
   "outputs": [],
   "source": [
    "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
    "\n",
    "if ARG_RESNET_NETWORK:\n",
    "    ResNet18, preprocess_input = Classifiers.get('resnet18')\n",
    "    base_model = ResNet18(input_shape=IMG_SHAPE, weights='imagenet', include_top=False)\n",
    "\n",
    "else:\n",
    "    ## Create the base model from the pre-trained model MobileNet V2\n",
    "    #base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "    #                                               # alpha=1.4,\n",
    "    #                                               include_top=False,\n",
    "    #      \n",
    "\n",
    "    \n",
    "    # Create the base model from the pre-trained model Inception V3\n",
    "\n",
    "    # When include_top=True and weights=None (random initialization), see below:\n",
    "    # __________________________________________________________________________________________________\n",
    "    # avg_pool (GlobalAveragePooling2 (None, 2048)         0           mixed10[0][0]                    \n",
    "    # __________________________________________________________________________________________________\n",
    "    # predictions (Dense)             (None, 1000)         2049000     avg_pool[0][0]                   \n",
    "    # ==================================================================================================        \n",
    "\n",
    "    #base_model = tf.keras.applications.InceptionV3(input_shape=IMG_SHAPE,\n",
    "    #                                               weights='imagenet',\n",
    "    #                                               include_top=False)\n",
    "\n",
    "    #base_model = tf.keras.applications.InceptionV3(input_shape=IMG_SHAPE,\n",
    "    # \n",
    "\n",
    "\n",
    "    base_model = tf.keras.applications.VGG16(input_shape=IMG_SHAPE,\n",
    "                                                   weights='imagenet',\n",
    "                                                   include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqcsxoJIEVXZ"
   },
   "source": [
    "This feature extractor converts each `160x160x3` image into a `5x5x1280` block of features. See what it does to the example batch of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y-2LJL0EEUcx"
   },
   "outputs": [],
   "source": [
    "feature_batch = base_model(image_batch)\n",
    "print(feature_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rlx56nQtfe8Y"
   },
   "source": [
    "## Feature extraction\n",
    "In this step, you will freeze the convolutional base created from the previous step and to use as a feature extractor. Additionally, you add a classifier on top of it and train the top-level classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CnMLieHBCwil"
   },
   "source": [
    "### Freeze the convolutional base\n",
    "\n",
    "It is important to freeze the convolutional base before you compile and train the model. Freezing (by setting layer.trainable = False) prevents the weights in a given layer from being updated during training. MobileNet V2 has many layers, so setting the entire model's trainable flag to False will freeze all the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OTCJH4bphOeo"
   },
   "outputs": [],
   "source": [
    "if not ARG_RESNET_NETWORK:\n",
    "    base_model.trainable = False\n",
    "else:\n",
    "    # resnet\n",
    "    for layer in base_model.layers:\n",
    "            if layer.__class__.__name__ != \"BatchNormalization\":\n",
    "                layer.trainable = False\n",
    "            else:\n",
    "                print(f\"{layer.name} ({layer.__class__.__name__})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KpbzSmPkDa-N"
   },
   "outputs": [],
   "source": [
    "# Let's take a look at the base model architecture\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7RFyBW06yFUC"
   },
   "outputs": [],
   "source": [
    "%cd '/content/fluence_maps'\n",
    "from add_regularization import add_regularization\n",
    "# adds a tf.keras.regularizers.l2(0.0001)\n",
    "#\"kernel_regularizer\":{\n",
    "#                        \"class_name\": \"L1L2\",\n",
    "#                        \"config\": {\n",
    "#                            \"l1\": 0,\n",
    "#                            \"l2\": 0.0001\n",
    "#                        }\n",
    "#                     }\n",
    "if add_regularizers:\n",
    "    base_model = add_regularization(base_model)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    for attr in ['kernel_regularizer']:\n",
    "        if hasattr(layer, attr):\n",
    "            if getattr(layer, attr) is None:\n",
    "                print('layer {} has no regularizer.'.format(layer.name))\n",
    "            else:\n",
    "                print('layer {} has a regularizer {}.'.format(layer.name, getattr(layer, attr)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WKjwYuIryFUE"
   },
   "outputs": [],
   "source": [
    "# from render_json import RenderJSON\n",
    "\n",
    "# RenderJSON(base_model.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "avX9AnbJyFUH"
   },
   "outputs": [],
   "source": [
    "# display the weights of some layers\n",
    "l = 1\n",
    "for layer in base_model.layers:\n",
    "    # print('layer {:3d}, name: {}'.format(l, layer.name))\n",
    "    if layer.name == \"expanded_conv_project\":\n",
    "        weights = layer.get_weights()\n",
    "        print(layer.get_config(), weights, weights[0].shape)\n",
    "    l += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dLnpMF5KOALm"
   },
   "outputs": [],
   "source": [
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "feature_batch_average = global_average_layer(feature_batch)\n",
    "print(feature_batch_average.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JzmSozfKDM6W"
   },
   "source": [
    "Stack the feature extractor, and these two layers using a `tf.keras.Sequential` model for network architectures other than ResNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wv4afXKj6cVa"
   },
   "outputs": [],
   "source": [
    "if not ARG_RESNET_NETWORK:\n",
    "    # Pablo March 10: add sigmoid\n",
    "    # WARNING: adding the activation function causes loss to keep close to 0.5 and does not decrease.\n",
    "    # prediction_layer = keras.layers.Dense(1, activation='sigmoid') # para obtener probabilidades y no logits\n",
    "\n",
    "    # https://keras.io/applications/#fine-tune-inceptionv3-on-a-new-set-of-classes\n",
    "    # https://stackoverflow.com/questions/58627411/how-to-use-inception-network-for-regression\n",
    "\n",
    "    # let's add a fully-connected layer\n",
    "    fc1 = keras.layers.Dense(512, activation='relu')\n",
    "    #fc2 = keras.layers.Dense(512, activation='relu')\n",
    "    #dropout = keras.layers.Dropout(rate=0.05) # no funciona\n",
    "    # and a linear output layer (regression)\n",
    "    prediction_layer = keras.layers.Dense(1, activation='linear')\n",
    "    # and a logistic layer -- let's say we have 200 classes (classification)\n",
    "    # prediction_layer = Dense(200, activation='softmax')(x)\n",
    "\n",
    "    # Up to March 27, 2020\n",
    "    # prediction_layer = keras.layers.Dense(1)\n",
    "    prediction_batch = prediction_layer(feature_batch_average)\n",
    "    print(prediction_batch.shape)\n",
    "\n",
    "    # Now stack the feature extractor, and these two layers using a `tf.keras.Sequential` model:\n",
    "    model = tf.keras.Sequential([\n",
    "      base_model,\n",
    "      global_average_layer,\n",
    "      fc1,\n",
    "      #dropout,\n",
    "      #fc2,\n",
    "      prediction_layer\n",
    "    ])\n",
    "    \n",
    "else:\n",
    "    avg = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "    out = tf.keras.layers.Dense(512, activation=\"relu\")(avg) # CHANGE to use avg pool only or avg pool + max pool\n",
    "    out = tf.keras.layers.BatchNormalization()(out) # bn after dropout or dense layer helps!\n",
    "    prediction_layer = tf.keras.layers.Dense(1, activation='linear')(out)\n",
    "    model = keras.models.Model(inputs=base_model.input, outputs=prediction_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0ylJXE_kRLi"
   },
   "source": [
    "### Compile the model\n",
    "\n",
    "You must compile the model before training it.  Since there are two classes, use a binary cross-entropy loss with `from_logits=True` since the model provides a linear output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RpR8HdyMhukJ"
   },
   "outputs": [],
   "source": [
    "base_learning_rate = 0.001\n",
    "# mse = square(y_true - y_pred)\n",
    "# mae = loss = abs(y_true - y_pred)\n",
    "# mape = 100 * abs(y_true - y_pred) / y_true\n",
    "# mae y mape son similares, no iguales, por eso tomo MAE que es el promedio de la diferencia absoluta entre el\n",
    "# gamma real y el gamma predicho\n",
    "#optimizer = tf.keras.optimizers.RMSprop(lr=base_learning_rate, momentum=0.95)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=base_learning_rate)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.MeanAbsoluteError(),\n",
    "              metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I8ARiyMFsgbH"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RxvgOYTDSWTx"
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hlHEavK7DUI7"
   },
   "outputs": [],
   "source": [
    "num_train = sum(1 for _ in raw_train)\n",
    "num_val = sum(1 for _ in raw_validation)\n",
    "print(f'Number of images in train partition: {num_train}, in validation: {num_val}.')\n",
    "if ARG_TEST_PARTITION:\n",
    "    num_test = sum(1 for _ in raw_test)\n",
    "    print(f'Number of images in test partiton: {num_test}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Om4O3EESkab1"
   },
   "outputs": [],
   "source": [
    "initial_epochs = 10\n",
    "steps_per_epoch = round(num_train)//BATCH_SIZE\n",
    "validation_steps=20\n",
    "\n",
    "# projects out just the first two components.\n",
    "tmp_validation_batches = validation_batches.map(lambda image, gamma, filename: (image, gamma))\n",
    "print(tmp_validation_batches)\n",
    "\n",
    "loss0 = mse0 = 0\n",
    "loss0, mse0 = model.evaluate(tmp_validation_batches, steps = validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8cYT1c48CuSd"
   },
   "outputs": [],
   "source": [
    "print(\"initial loss: {:.2f}\".format(loss0))\n",
    "print(\"initial mape: {:.2f}\".format(mse0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JsaRFlZ9B6WK"
   },
   "outputs": [],
   "source": [
    "# projects out just the first two components.\n",
    "tmp_train_batches = train_batches.map(lambda image, gamma, filename: (image, gamma))\n",
    "print(tmp_train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XTtp6LG7yFUw"
   },
   "outputs": [],
   "source": [
    "# Implement callback function to stop training\n",
    "class MyCallback(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, wait_epochs):\n",
    "        self.__wait_epochs = wait_epochs\n",
    "        self.__latest_peak_mae = 4.0\n",
    "        self.__waited_epochs = 0\n",
    "        self.__saved_model_file = None\n",
    "        self.__saved_model = None\n",
    "        \n",
    "    def stopTraining(self, epoch, val_mae):\n",
    "        stop_early = False\n",
    "        # check for early stop\n",
    "        if val_mae < self.__latest_peak_mae:\n",
    "            self.__latest_peak_mae = val_mae\n",
    "            print(f\"\\nNew peak val_mae reached: {val_mae:6.4}\")\n",
    "\n",
    "            t = time.time()\n",
    "            dir = os.path.join(ARG_DATASET_DIR, \"models\")\n",
    "            save_model_path = \"{}/{}.h5\".format(dir, int(t))\n",
    "            print(save_model_path)\n",
    "            # Either 'tf' or 'h5', indicating whether to save the model to Tensorflow SavedModel or HDF5. Defaults to 'tf' \n",
    "            # in TF 2.X, and 'h5' in TF 1.X.            \n",
    "            model.save(save_model_path, save_format='h5')\n",
    "            # borro el archivo del modelo anterior\n",
    "            if self.__saved_model_file is not None:\n",
    "                os.remove(self.__saved_model_file)\n",
    "            self.__saved_model_file = save_model_path\n",
    "            reloaded_model = tf.keras.models.load_model(save_model_path)\n",
    "            self.__saved_model = reloaded_model\n",
    "\n",
    "        if val_mae > self.__latest_peak_mae:\n",
    "            # Si llevo N+ epochs sin mejora\n",
    "            if self.__waited_epochs >= self.__wait_epochs:\n",
    "                print(\"\\nStopping early at epoch {0} with saved peak mae {1:10.8}\"\n",
    "                      .format(epoch + 1, self.__latest_peak_mae))\n",
    "                stop_early = True\n",
    "            \n",
    "            self.__waited_epochs += 1\n",
    "            \n",
    "        else:\n",
    "            self.__latest_peak_mae = val_mae\n",
    "            # Reset waited epochs.\n",
    "            self.__waited_epochs = 0\n",
    "            \n",
    "        return stop_early\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # print('\\nTraining: epoch {} ends at {}'.format(epoch, datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")))\n",
    "        if self.stopTraining(epoch, logs.get('val_loss')):\n",
    "            self.model.stop_training = True\n",
    "\n",
    "    @property\n",
    "    def saved_model_file(self):\n",
    "        return self.__saved_model_file\n",
    "    \n",
    "    @property\n",
    "    def saved_model(self):\n",
    "        return self.__saved_model\n",
    "\n",
    "    def reset_waited_epochs(self):\n",
    "        self.__waited_epochs = 0;\n",
    "    \n",
    "\n",
    "# Instantiate a callback object\n",
    "callbackObj = MyCallback(ARG_EPOCHS_WO_IMPROVEMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MZyPn887yFUz"
   },
   "outputs": [],
   "source": [
    "history = model.fit(tmp_train_batches,\n",
    "                    epochs=initial_epochs,\n",
    "                    validation_data=tmp_validation_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hd94CKImf8vi"
   },
   "source": [
    "### Learning curves\n",
    "\n",
    "Let's take a look at the learning curves of the training and validation accuracy/loss when using the MobileNet V2 base model as a fixed feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zvmYWOgoyFU4"
   },
   "outputs": [],
   "source": [
    "print(history.epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Po3M6UrGvKr"
   },
   "outputs": [],
   "source": [
    "# Pablo Feb 25:\n",
    "# Python never implicitly copies objects. When you set list2 = list1, you are making them refer to the same exact\n",
    "# list object, so when you mutate it, all references to it keep referring to the object in its current state.\n",
    "mae = history.history['mse'].copy()\n",
    "val_mae = history.history['val_mse'].copy()\n",
    "\n",
    "loss = history.history['loss'].copy()\n",
    "val_loss = history.history['val_loss'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "53OTCh3jnbwV"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(mae, label='Training MSE')\n",
    "plt.plot(val_mae, label='Validation MSE')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('MSE')\n",
    "plt.ylim([0,50])\n",
    "plt.title('Training and Validation MSE')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim([0,10])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CqwV-CRdS6Nv"
   },
   "source": [
    "## Fine tuning\n",
    "In the feature extraction experiment, you were only training a few layers on top of a VGG16 base model. The weights of the pre-trained network were **not** updated during training.\n",
    "\n",
    "One way to increase performance even further is to train (or \"fine-tune\") the weights of the top layers of the pre-trained model alongside the training of the classifier you added. The training process will force the weights to be tuned from generic feature maps to features associated specifically with the dataset.\n",
    "\n",
    "Note: This should only be attempted after you have trained the top-level classifier with the pre-trained model set to non-trainable. If you add a randomly initialized classifier on top of a pre-trained model and attempt to train all layers jointly, the magnitude of the gradient updates will be too large (due to the random weights from the classifier) and your pre-trained model will forget what it has learned.\n",
    "\n",
    "Also, you should try to fine-tune a small number of top layers rather than the whole VGG16 model. In most convolutional networks, the higher up a layer is, the more specialized it is. The first few layers learn very simple and generic features that generalize to almost all types of images. As you go higher up, the features are increasingly more specific to the dataset on which the model was trained. The goal of fine-tuning is to adapt these specialized features to work with the new dataset, rather than overwrite the generic learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CPXnzUK0QonF"
   },
   "source": [
    "### Un-freeze the top layers of the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfxv_ifotQak"
   },
   "source": [
    "All you need to do is unfreeze the `base_model` and set the bottom layers to be un-trainable. Then, you should recompile the model (necessary for these changes to take effect), and resume training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4nzcagVitLQm"
   },
   "outputs": [],
   "source": [
    "base_model.trainable = True\n",
    "\n",
    "# Pablo Feb 25: con este loop me aseguro que las capas que en la próxima celda *NO* pongo como layer.trainable = False sean\n",
    "# trainable.\n",
    "l = 1\n",
    "for layer in base_model.layers:\n",
    "    print('layer {:3d}, name: {}'.format(l, layer.name))\n",
    "    if layer.__class__.__name__ != \"BatchNormalization\":\n",
    "        layer.trainable =  True\n",
    "    l += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-4HgVAacRs5v"
   },
   "outputs": [],
   "source": [
    "# Let's take a look to see how many layers are in the base model\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    if layer.__class__.__name__ != \"BatchNormalization\":\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Uk1dgsxT0IS"
   },
   "source": [
    "### Compile the model\n",
    "\n",
    "Compile the model using a much lower learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AVvlrXb9yFVH"
   },
   "outputs": [],
   "source": [
    "# This function keeps the learning rate at 0.001 for the first ten epochs\n",
    "# and decreases it exponentially after that.\n",
    "def lr_scheduler(epoch):\n",
    "    lr = base_learning_rate\n",
    "    #k = 0.15\n",
    "    #if epoch >= 10 and epoch <= 40:\n",
    "    #    lr = tf.math.exp(k * (10 - epoch)) * base_learning_rate\n",
    "    #    print(\"\\nlearning rate: %.8f\"%(lr))\n",
    "    if epoch >= 10 and epoch <= 30:\n",
    "        lr = base_learning_rate/10\n",
    "    elif epoch > 30 and epoch <= 60:\n",
    "        lr = base_learning_rate/100\n",
    "    elif epoch > 60:\n",
    "        lr = base_learning_rate/500\n",
    "    print(\"\\nlearning rate: %.6f\"%(lr))\n",
    "    \n",
    "    return lr\n",
    "    \n",
    "    \n",
    "callbackLR = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NtUnaz0WUDva"
   },
   "outputs": [],
   "source": [
    "# optimizer = tf.keras.optimizers.RMSprop(lr=base_learning_rate/10, momentum=0.95)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=base_learning_rate/10, epsilon=1e-08, amsgrad=False)\n",
    "model.compile(loss=tf.keras.losses.MeanAbsoluteError(),\n",
    "              optimizer = optimizer,\n",
    "              metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WwBWy7J2kZvA"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bNXelbMQtonr"
   },
   "outputs": [],
   "source": [
    "len(model.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4G5O4jd6TuAG"
   },
   "source": [
    "### Continue training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NKrhmlnv-4_M"
   },
   "outputs": [],
   "source": [
    "# Crear el path donde se va a ir guardando el mejor mejor modelo en training.\n",
    "#dirName = \"./tmp/saved_models/\"\n",
    "#if not os.path.exists(dirName):\n",
    "#    os.makedirs(dirName)\n",
    "#    print(\"Directory\" , dirName ,  \"Created.\")\n",
    "#else:    \n",
    "#    print(\"Directory\" , dirName ,  \"already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0foWUN-yDLo_"
   },
   "source": [
    "If you trained to convergence earlier, this step will improve your accuracy by a few percentage points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ECQLkAsFTlun"
   },
   "outputs": [],
   "source": [
    "fine_tune_epochs = ARG_MAX_FINE_TUNING_EPOCHS\n",
    "total_epochs =  (history.epoch[-1]+1) + fine_tune_epochs\n",
    "\n",
    "history_fine = model.fit(tmp_train_batches,\n",
    "                         epochs=total_epochs,\n",
    "                         initial_epoch =  history.epoch[-1]+1,\n",
    "                         validation_data=tmp_validation_batches,\n",
    "                         callbacks=[callbackObj, callbackLR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vjiNEG47yFVY"
   },
   "outputs": [],
   "source": [
    "print(history_fine.epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PpA8PlpQKygw"
   },
   "outputs": [],
   "source": [
    "mae += history_fine.history['mse']\n",
    "val_mae += history_fine.history['val_mse']\n",
    "\n",
    "loss += history_fine.history['loss']\n",
    "val_loss += history_fine.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Igm8aMrLyFVj"
   },
   "outputs": [],
   "source": [
    "mae = mae[1:]\n",
    "val_mae = val_mae[1:]\n",
    "loss = loss[1:]\n",
    "val_loss = val_loss[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "chW103JUItdk"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(mae, label='Training MSE')\n",
    "plt.plot(val_mae, label='Validation MSE')\n",
    "plt.ylim([0, 4])\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "          plt.ylim(), label='Fine Tuning starts')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Training and Validation MSE')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.ylim([0, 5])\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "         plt.ylim(), label='Fine Tuning Starts')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TZTwG7nhm0C"
   },
   "source": [
    "## Summary:\n",
    "\n",
    "* **Using a pre-trained model for feature extraction**:  When working with a small dataset, it is a common practice to take advantage of features learned by a model trained on a larger dataset in the same domain. This is done by instantiating the pre-trained model and adding a fully-connected classifier on top. The pre-trained model is \"frozen\" and only the weights of the classifier get updated during training.\n",
    "In this case, the convolutional base extracted all the features associated with each image and you just trained a classifier that determines the image class given that set of extracted features.\n",
    "\n",
    "* **Fine-tuning a pre-trained model**: To further improve performance, one might want to repurpose the top-level layers of the pre-trained models to the new dataset via fine-tuning.\n",
    "In this case, you tuned your weights such that your model learned high-level features specific to the dataset. This technique is usually recommended when the training dataset is large and very similar to the original dataset that the pre-trained model was trained on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eQy9mVkuoFWj"
   },
   "outputs": [],
   "source": [
    "!ls -l \"/content/drive/MyDrive/Healthcare/Radioterapia/data/ciolaplata/models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f4tr0VXGyFVr"
   },
   "outputs": [],
   "source": [
    "print(f'Best saved model file = {callbackObj.saved_model_file}')\n",
    "saved_model = callbackObj.saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "esaw0xjhyFVv"
   },
   "outputs": [],
   "source": [
    "# Evaluate the saved model on the train set which differs from values reported during training\n",
    "loss, mse = saved_model.evaluate(tmp_train_batches, verbose=2)\n",
    "print(\"Saved model, train loss: {:5.4f}\".format(loss))\n",
    "print('Saved model, train mse: {:5.4f}'.format(mse))\n",
    "\n",
    "# Evaluate the saved model on the validation set which differs from values reported during training\n",
    "loss, mse = saved_model.evaluate(tmp_validation_batches, verbose=2)\n",
    "print(\"\\nSaved model, validation loss: {:5.4f}\".format(loss))\n",
    "print('Saved model, validation mse: {:5.4f}'.format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qACCSCEAyFVz"
   },
   "outputs": [],
   "source": [
    "#result_batch = model.predict(tmp_train_batches)\n",
    "#reloaded_result_batch = reloaded_model.predict(tmp_train_batches)\n",
    "#print(abs(reloaded_result_batch - result_batch).max())\n",
    "#np.testing.assert_allclose(result_batch, reloaded_result_batch, rtol=1e-6, atol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kcEpqDWFyFV4"
   },
   "outputs": [],
   "source": [
    "# projects out just the first two components.\n",
    "if ARG_TEST_PARTITION:\n",
    "    tmp_test_batches = test_batches.map(lambda image, gamma, filename: (image, gamma))\n",
    "    print(tmp_test_batches)\n",
    "\n",
    "    # Evaluate the reloaded model on the test set\n",
    "    loss, mae = saved_model.evaluate(tmp_test_batches, verbose=1)\n",
    "    print(\"\\n\\nSaved model, test loss: {:5.4f}\".format(loss))\n",
    "    print('Saved model, test mae: {:5.4f}'.format(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R1oRiBayRPVt"
   },
   "outputs": [],
   "source": [
    "drive.flush_and_unmount()\n",
    "print('All changes made in this colab session should now be visible in Drive.')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "transfer_learning.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
