{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/pablojrios/fluence_maps/blob/master/create_tfrecords/tf2_create_tfrecords.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "m6sGBgOzA6NY"
   },
   "outputs": [],
   "source": [
    "def isGoogleColab():\n",
    "    # 'ipykernel.zmqshell' runs in our server\n",
    "    # 'google.colab._shell' runs in Google Colab\n",
    "    return get_ipython().__class__.__module__ == 'google.colab._shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rvsvrcRHb7b3"
   },
   "outputs": [],
   "source": [
    "# import lodgepole.image_tools as lit doesn't work, the following is equivalent\n",
    "# from importlib.machinery import SourceFileLoader\n",
    "# somemodule = SourceFileLoader('lit', '/content/lodgepole/lodgepole/image_tools.py').load_module()\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import KFold\n",
    "from os import path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n_KVdeEFcCVd",
    "outputId": "e38295b2-3883-40e1-cf92-813c156f5de8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version = 2.2.0\n",
      "Executing eagerly = True\n"
     ]
    }
   ],
   "source": [
    "print('Tensorflow version = {}'.format(tf.__version__))\n",
    "print('Executing eagerly = {}'.format(tf.executing_eagerly()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xJx5w948nCcj",
    "outputId": "604a1fe7-f602-40d8-ecf0-7d973e4ccd4a"
   },
   "outputs": [],
   "source": [
    "if isGoogleColab():\n",
    "    # if os.path.exists('lodgepole'):\n",
    "    #     !rm -fr lodgepole\n",
    "\n",
    "    # !git clone https://gitlab.com/brohrer/lodgepole.git\n",
    "    # !pip install -e lodgepole\n",
    "\n",
    "    %cd -q '/content'\n",
    "    if os.path.exists('fluence_maps'):\n",
    "        !rm -fr fluence_maps\n",
    "\n",
    "    ## Install required dependencies\n",
    "    !pip install -q pydicom\n",
    "\n",
    "    GIT_USERNAME = \"pablojrios\"\n",
    "    GIT_TOKEN = \"1d88a0b85d2b00a03796e4d8b7e5f7b249b12f9b\"\n",
    "    !git clone -s https://{GIT_TOKEN}@github.com/{GIT_USERNAME}/fluence_maps.git\n",
    "\n",
    "    %cd -q '/content/fluence_maps/create_tfrecords'\n",
    "        \n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "       \n",
    "    ARG_DATASET_DIR='/content/drive/My Drive/Healthcare/Radioterapia/data/ciolaplata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RNvX7kV3Bd9Q"
   },
   "outputs": [],
   "source": [
    "from dataset_utils import _dataset_exists, _get_filenames_and_gamma_values, _convert_dataset\n",
    "from tf2_oversampling_dicom_files import do_oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1wgXL1PXBhEt"
   },
   "outputs": [],
   "source": [
    "# ===============================================DEFINE YOUR ARGUMENTS=================================================\n",
    "if not isGoogleColab():\n",
    "    ARG_DATASET_DIR='/hdd/data/radioterapia/ciolaplata'\n",
    "# The number of shards to split the dataset into\n",
    "ARG_NUM_SHARDS=1\n",
    "ARG_VALIDATION_SIZE=0.2\n",
    "# if False only training and validation partition are created.\n",
    "ARG_TEST_PARTITION=False\n",
    "# if cross validation is enabled then ARG_VALIDATION_SIZE and ARG_TEST_PARTITION are *not* taken into account.\n",
    "# Further, oversampling (ARG_OVERSAMPLING) is not supported when creating a dataset for cross-validation.\n",
    "ARG_CROSS_VALIDATION_ENABLED=True\n",
    "# num folds cross validation (default is 5). Only taken into account if cross validation is enabled\n",
    "ARG_NUM_CV_FOLDS=5\n",
    "# Seed for repeatability.\n",
    "ARG_RANDOM_SEED=23456\n",
    "# folder under ARG_DATASET_DIR path.\n",
    "ARG_TFDATASET_FOLDER=f'tfds.2019-2018-2017.localnorm.DS10%.{ARG_RANDOM_SEED}.gammaGT95.unsersampled'\n",
    "# 0.1172 para 500 casos, 0.3515 para 1500 casos\n",
    "ARG_SAMPLE_DATASET=1596/(7245-1596) # >0, 1.0 uses all cases (no sample is taken)\n",
    "# file with gamma values under ARG_DATASET_DIR path.\n",
    "# ARG_DICOM_AND_GAMMA_CSV='codex.2018-2019.csv' # mapas 3mm/3% con doseshift años 2018 y 2019\n",
    "# ARG_DICOM_AND_GAMMA_CSV = \"codex-2019-3mm3%-doseshift.csv\" # mapas 3mm/3% con doseshift año 2019\n",
    "# ARG_DICOM_AND_GAMMA_CSV = \"codex-2019-2mm2%-doseshift.csv\" # mapas 2mm/2% con doseshift año 2019\n",
    "# ARG_DICOM_AND_GAMMA_CSV = \"codex-2019-3mm3%-doseshift-TR40%.csv\" # mapas con ajuste tolerancia 40%\n",
    "# ARG_DICOM_AND_GAMMA_CSV = \"codex-2019-3mm3%-doseshift-40TH-localnorm.csv\" # mapas CODEX 3mm 3% 40TH Local Norm año 2019\n",
    "# ARG_DICOM_AND_GAMMA_CSV = \"codex-2019-3mm3%-doseshift-40TH-localnorm-nooutliers.csv\" # mapas CODEX 3mm 3% 40TH Local Norm año 2019 (remuevo outliers con gamma <= 65%, 6 en total)\n",
    "# ARG_DICOM_AND_GAMMA_CSV = \"codex.2019.3mm3%Doseshift10%-localnorm-sinoutliers74.csv\"\n",
    "# ARG_DICOM_AND_GAMMA_CSV = \"codex.2018-2019.3mm3%Doseshift10%-localnorm-sinoutliers74.csv\"\n",
    "# ARG_DICOM_AND_GAMMA_CSV = \"codex.2018-2019-3mm3%Doseshift10%-localnorm-undersampling95.csv\"\n",
    "ARG_DICOM_AND_GAMMA_CSV = \"codex.2019-2018-2017.3mm3%Doseshift10%-localnorm-sinoutliers74.csv\"\n",
    "# ARG_DICOM_AND_GAMMA_CSV = \"codex.2017.3mm3%Doseshift10%-localnorm-sinoutliers74.csv\"\n",
    "# ARG_DICOM_AND_GAMMA_CSV = \"codex.2019-2017.3mm3%Doseshift10%-localnorm-sinoutliers74.csv\" # 4269 casos\n",
    "# ARG_IMAGE_TYPE: 0 - RGB; 1 - Grayscale: Convert color images to 3D grayscale images (channel is repeated 3 times);\n",
    "# 2 - Dicom\n",
    "ARG_IMAGE_TYPE=2\n",
    "# if True copy of images is performed.\n",
    "ARG_OVERSAMPLING=False\n",
    "ARG_OVERSAMPLING_GAMMA_THRESHOLD = 97.0 # percentage\n",
    "ARG_OVERSAMPLING_FACTOR = 3.0 # 1 is 100%\n",
    "\n",
    "ARG_INCLUDE_GAMMA=True\n",
    "ARG_INCLUDE_GAMMA_VALUE=95.0\n",
    "# True: exclude maps with gammas >= ARG_FILTER_GAMMA_VALUE (i.e.: train a model with problematic gammas)\n",
    "# False exclude maps with gammas < ARG_FILTER_GAMMA_VALUE\n",
    "ARG_INCLUDE_GAMMA_LOWERTHAN=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OWE93zltBnR0",
    "outputId": "8487b549-fe17-43e1-a38d-d4a375e3b419"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2046, 2)\n",
      "                                    dicom_full_filepath  gamma_index\n",
      "5987  /hdd/data/radioterapia/ciolaplata/2018/1.3.6.1...      98.5301\n",
      "1607  /hdd/data/radioterapia/ciolaplata/2017/1.3.6.1...      97.5114\n",
      "6181  /hdd/data/radioterapia/ciolaplata/2018/1.3.6.1...      98.7316\n",
      "5497  /hdd/data/radioterapia/ciolaplata/2018/1.3.6.1...      97.9247\n",
      "5270  /hdd/data/radioterapia/ciolaplata/2018/1.3.6.1...      97.4779\n",
      "5451  /hdd/data/radioterapia/ciolaplata/2018/1.3.6.1...      97.8391\n",
      "4561  /hdd/data/radioterapia/ciolaplata/2018/1.3.6.1...      93.0490\n",
      "5981  /hdd/data/radioterapia/ciolaplata/2018/1.3.6.1...      98.5240\n",
      "3673  /hdd/data/radioterapia/ciolaplata/2019/1.3.6.1...      88.1338\n",
      "6742  /hdd/data/radioterapia/ciolaplata/2018/1.3.6.1...      99.2280\n",
      "(1588, 2)\n"
     ]
    }
   ],
   "source": [
    "#=================================================CHECKS==============================================\n",
    "# Check if there is a dataset directory entered\n",
    "if ARG_DATASET_DIR == \"\":\n",
    "    raise ValueError('dataset_dir is empty. Please state a dataset_dir argument.')\n",
    "    \n",
    "if ARG_TFDATASET_FOLDER == \"\":\n",
    "    raise ValueError('tfdataset_folder is empty. Please state a tfdataset_dir argument.')\n",
    "    \n",
    "# If the TFRecord files already exist in the directory, then exit without creating the files again\n",
    "tfdataset_dir = path.join(ARG_DATASET_DIR, ARG_TFDATASET_FOLDER)\n",
    "if _dataset_exists(dataset_dir = tfdataset_dir, _NUM_SHARDS = ARG_NUM_SHARDS):\n",
    "    print(f'Dataset files already exist in {tfdataset_dir}. Exiting without re-creating them.')\n",
    "    sys.exit()\n",
    "\n",
    "if not (0 < ARG_SAMPLE_DATASET <= 1.0):\n",
    "    print(f'Wrong value for input param ARG_SAMPLE_DATASET: {ARG_SAMPLE_DATASET}')\n",
    "    sys.exit()\n",
    "    \n",
    "elif not tf.io.gfile.exists(tfdataset_dir):\n",
    "    tf.io.gfile.mkdir(tfdataset_dir)\n",
    "#==============================================END OF CHECKS==========================================\n",
    "\n",
    "# Get a pandas dataframe of image full filenames and gamma indeces values.\n",
    "df_dcm_out = _get_filenames_and_gamma_values(ARG_DICOM_AND_GAMMA_CSV, ARG_DATASET_DIR,\n",
    "                                             sample=ARG_SAMPLE_DATASET, seed=ARG_RANDOM_SEED)\n",
    "print(df_dcm_out.shape)\n",
    "print(df_dcm_out.head(10))\n",
    "if ARG_INCLUDE_GAMMA:\n",
    "    if ARG_INCLUDE_GAMMA_LOWERTHAN:\n",
    "        df_dcm_out = df_dcm_out.loc[df_dcm_out['gamma_index'] < ARG_INCLUDE_GAMMA_VALUE]\n",
    "    else:\n",
    "        df_dcm_out = df_dcm_out.loc[df_dcm_out['gamma_index'] >= ARG_INCLUDE_GAMMA_VALUE]\n",
    "    print(df_dcm_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataset(tfdataset_dir, training_filenames, training_gamma, validation_filenames, validation_gamma,\n",
    "                  testing_filenames=None, testing_gamma=None):\n",
    "    \n",
    "    start = time.time()\n",
    "    print(\"Writing TF records to {}\".format(tfdataset_dir))\n",
    "    \n",
    "    # First, convert the training and validation sets.\n",
    "    _convert_dataset('train', training_filenames, training_gamma,\n",
    "                      dataset_dir = tfdataset_dir, _NUM_SHARDS = ARG_NUM_SHARDS, image_type = ARG_IMAGE_TYPE)\n",
    "\n",
    "    if (ARG_CROSS_VALIDATION_ENABLED or len(validation_filenames) > 0):\n",
    "        _convert_dataset('validation', validation_filenames, validation_gamma,\n",
    "                          dataset_dir = tfdataset_dir, _NUM_SHARDS = ARG_NUM_SHARDS, image_type = ARG_IMAGE_TYPE)\n",
    "\n",
    "        if not ARG_CROSS_VALIDATION_ENABLED and ARG_TEST_PARTITION:\n",
    "            _convert_dataset('test', testing_filenames, testing_gamma,\n",
    "                              dataset_dir = tfdataset_dir, _NUM_SHARDS = ARG_NUM_SHARDS, image_type = ARG_IMAGE_TYPE)\n",
    "\n",
    "    end = time.time() - start\n",
    "    print(f'Finished converting the dataset in {end:.2f} seconds.')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_build_simple_partition():\n",
    "    global df_dcm_out\n",
    "    \n",
    "    print(\"Create simple partition dataset.\")\n",
    "    \n",
    "    print(\"Reading images from {}\".format(tfdataset_dir))\n",
    "\n",
    "    # Find the number of validation examples we need\n",
    "    num_validation = int(ARG_VALIDATION_SIZE * len(df_dcm_out))\n",
    "\n",
    "    print(f'\\nNum. training images = {len(df_dcm_out) - num_validation}, num. validation images = {num_validation}')\n",
    "\n",
    "    print(f'\\nrandom seed partition = {ARG_RANDOM_SEED}')\n",
    "    # Divide the training datasets into train and test:\n",
    "    df_dcm_out = shuffle(df_dcm_out, random_state=ARG_RANDOM_SEED)\n",
    "\n",
    "    # convert to list because a dataframe column is of type pandas...Series\n",
    "    if not ARG_TEST_PARTITION:\n",
    "        df_training = df_dcm_out[num_validation:]\n",
    "        df_validation = df_dcm_out[:num_validation]\n",
    "    else:\n",
    "        df_training = df_dcm_out[num_validation*2:]\n",
    "        df_validation = df_dcm_out[:num_validation]\n",
    "        df_testing = df_dcm_out[num_validation:num_validation*2]\n",
    "\n",
    "    # Hacer oversampling de mapas menores o iguales a un valor de gamma en df_training\n",
    "    if ARG_OVERSAMPLING:\n",
    "        df_training = do_oversampling(df_training, ARG_OVERSAMPLING_GAMMA_THRESHOLD, ARG_OVERSAMPLING_FACTOR)\n",
    "\n",
    "    # convert to list because a dataframe column is of type pandas...Series\n",
    "    if not ARG_TEST_PARTITION:\n",
    "        training_filenames = df_training['dicom_full_filepath'].to_list()\n",
    "        validation_filenames = df_validation['dicom_full_filepath'].to_list()\n",
    "        training_gamma = df_training['gamma_index'].to_list()\n",
    "        validation_gamma = df_validation['gamma_index'].to_list()\n",
    "        write_dataset(tfdataset_dir, training_filenames, training_gamma, validation_filenames, validation_gamma)\n",
    "    \n",
    "    else:\n",
    "        training_filenames = df_training['dicom_full_filepath'].to_list()\n",
    "        validation_filenames = df_validation['dicom_full_filepath'].to_list()\n",
    "        testing_filenames = df_testing['dicom_full_filepath'].to_list()\n",
    "        training_gamma = df_training['gamma_index'].to_list()\n",
    "        validation_gamma = df_validation['gamma_index'].to_list()\n",
    "        testing_gamma = df_testing['gamma_index'].to_list()\n",
    "        write_dataset(tfdataset_dir, training_filenames, training_gamma, validation_filenames, validation_gamma,\n",
    "                      testing_filenames, testing_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_build_cv_partition():\n",
    "    global df_dcm_out\n",
    "    \n",
    "    print(f\"Create dataset for cross-validation with k={ARG_NUM_CV_FOLDS}\")\n",
    "    \n",
    "    print(\"Reading images from {}\".format(ARG_DATASET_DIR))\n",
    "    print(f'Total images = {len(df_dcm_out)}')\n",
    "    print(f'random seed partition = {ARG_RANDOM_SEED}')\n",
    "    \n",
    "    # shuffle added when we merge 2108 and 2019, it wasn't done with 2019 only maps.\n",
    "    df_dcm_out = shuffle(df_dcm_out, random_state=ARG_RANDOM_SEED)\n",
    "    \n",
    "    filenames = df_dcm_out['dicom_full_filepath']\n",
    "    gamma = df_dcm_out['gamma_index']\n",
    "        \n",
    "    k_fold = KFold(ARG_NUM_CV_FOLDS, shuffle=True, random_state=ARG_RANDOM_SEED)\n",
    "    for k, (train, valid) in enumerate(k_fold.split(filenames, gamma)):\n",
    "\n",
    "        print('first 5 filenames from training and validation:')\n",
    "        print(filenames.iloc[train].head(5))\n",
    "        print(filenames.iloc[valid].head(5))\n",
    "        \n",
    "        # preseleccion de genes con todos los casos (anteriormente se hizo en la partición de training)\n",
    "        training_filenames = filenames.iloc[train].tolist()\n",
    "        training_gamma = gamma.iloc[train].tolist()\n",
    "        validation_filenames = filenames.iloc[valid].tolist()\n",
    "        validation_gamma = gamma.iloc[valid].tolist()\n",
    "                \n",
    "        print(f'\\nfold={k}, train size={len(training_filenames)}, validation size={len(validation_filenames)}')\n",
    "\n",
    "        tfdataset_fold_dir = tfdataset_dir + \".fold\" + str(k)\n",
    "        if not tf.io.gfile.exists(tfdataset_fold_dir):\n",
    "            tf.io.gfile.mkdir(tfdataset_fold_dir)\n",
    "        \n",
    "        write_dataset(tfdataset_fold_dir, training_filenames, training_gamma, validation_filenames, validation_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create dataset for cross-validation with k=5\n",
      "Reading images from /hdd/data/radioterapia/ciolaplata\n",
      "Total images = 1588\n",
      "random seed partition = 23456\n",
      "first 5 filenames from training and validation:\n",
      "2384    /hdd/data/radioterapia/ciolaplata/2017/1.3.6.1...\n",
      "2116    /hdd/data/radioterapia/ciolaplata/2017/1.3.6.1...\n",
      "2088    /hdd/data/radioterapia/ciolaplata/2017/1.3.6.1...\n",
      "2342    /hdd/data/radioterapia/ciolaplata/2017/1.3.6.1...\n",
      "4242    /hdd/data/radioterapia/ciolaplata/2019/1.3.6.1...\n",
      "Name: dicom_full_filepath, dtype: object\n",
      "1613    /hdd/data/radioterapia/ciolaplata/2017/1.3.6.1...\n",
      "1914    /hdd/data/radioterapia/ciolaplata/2017/1.3.6.1...\n",
      "1580    /hdd/data/radioterapia/ciolaplata/2017/1.3.6.1...\n",
      "1682    /hdd/data/radioterapia/ciolaplata/2017/1.3.6.1...\n",
      "6181    /hdd/data/radioterapia/ciolaplata/2018/1.3.6.1...\n",
      "Name: dicom_full_filepath, dtype: object\n",
      "\n",
      "fold=0, train size=1270, validation size=318\n",
      "Writing TF records to /hdd/data/radioterapia/ciolaplata/tfds.2019-2018-2017.localnorm.DS10%.23456.gammaGT95.unsersampled.fold0\n",
      ">> Image 1270/1270 with 4190 bytes in shard 0 converted\n",
      ">> Image 318/318 with 4598 bytes in shard 0 converted\n",
      "Finished converting the dataset in 4.31 seconds.\n",
      "first 5 filenames from training and validation:\n",
      "2116    /hdd/data/radioterapia/ciolaplata/2017/1.3.6.1...\n",
      "1613    /hdd/data/radioterapia/ciolaplata/2017/1.3.6.1...\n",
      "2342    /hdd/data/radioterapia/ciolaplata/2017/1.3.6.1...\n",
      "1914    /hdd/data/radioterapia/ciolaplata/2017/1.3.6.1...\n",
      "6620    /hdd/data/radioterapia/ciolaplata/2018/1.3.6.1...\n",
      "Name: dicom_full_filepath, dtype: object\n",
      "2384    /hdd/data/radioterapia/ciolaplata/2017/1.3.6.1...\n",
      "2088    /hdd/data/radioterapia/ciolaplata/2017/1.3.6.1...\n",
      "4242    /hdd/data/radioterapia/ciolaplata/2019/1.3.6.1...\n",
      "6586    /hdd/data/radioterapia/ciolaplata/2018/1.3.6.1...\n",
      "4764    /hdd/data/radioterapia/ciolaplata/2018/1.3.6.1...\n",
      "Name: dicom_full_filepath, dtype: object\n",
      "\n",
      "fold=1, train size=1270, validation size=318\n",
      "Writing TF records to /hdd/data/radioterapia/ciolaplata/tfds.2019-2018-2017.localnorm.DS10%.23456.gammaGT95.unsersampled.fold1\n",
      ">> Image 1270/1270 with 4459 bytes in shard 0 converted\n",
      ">> Image 318/318 with 4190 bytes in shard 0 converted\n",
      "Finished converting the dataset in 4.39 seconds.\n",
      "first 5 filenames from training and validation:\n",
      "2384    /hdd/data/radioterapia/ciolaplata/2017/1.3.6.1...\n",
      "2116    /hdd/data/radioterapia/ciolaplata/2017/1.3.6.1...\n",
      "1613    /hdd/data/radioterapia/ciolaplata/2017/1.3.6.1...\n",
      "2088    /hdd/data/radioterapia/ciolaplata/2017/1.3.6.1...\n",
      "2342    /hdd/data/radioterapia/ciolaplata/2017/1.3.6.1...\n",
      "Name: dicom_full_filepath, dtype: object\n",
      "6620    /hdd/data/radioterapia/ciolaplata/2018/1.3.6.1...\n",
      "5778    /hdd/data/radioterapia/ciolaplata/2018/1.3.6.1...\n",
      "3100    /hdd/data/radioterapia/ciolaplata/2019/1.3.6.1...\n",
      "6721    /hdd/data/radioterapia/ciolaplata/2018/1.3.6.1...\n",
      "6418    /hdd/data/radioterapia/ciolaplata/2018/1.3.6.1...\n",
      "Name: dicom_full_filepath, dtype: object\n",
      "\n",
      "fold=2, train size=1270, validation size=318\n",
      "Writing TF records to /hdd/data/radioterapia/ciolaplata/tfds.2019-2018-2017.localnorm.DS10%.23456.gammaGT95.unsersampled.fold2\n",
      ">> Image 1270/1270 with 4190 bytes in shard 0 converted\n",
      ">> Image 318/318 with 3731 bytes in shard 0 converted\n",
      "Finished converting the dataset in 4.43 seconds.\n",
      "first 5 filenames from training and validation:\n",
      "2384    /hdd/data/radioterapia/ciolaplata/2017/1.3.6.1...\n",
      "1613    /hdd/data/radioterapia/ciolaplata/2017/1.3.6.1...\n",
      "2088    /hdd/data/radioterapia/ciolaplata/2017/1.3.6.1...\n",
      "4242    /hdd/data/radioterapia/ciolaplata/2019/1.3.6.1...\n",
      "1914    /hdd/data/radioterapia/ciolaplata/2017/1.3.6.1...\n",
      "Name: dicom_full_filepath, dtype: object\n",
      "2116    /hdd/data/radioterapia/ciolaplata/2017/1.3.6.1...\n",
      "2342    /hdd/data/radioterapia/ciolaplata/2017/1.3.6.1...\n",
      "2170    /hdd/data/radioterapia/ciolaplata/2017/1.3.6.1...\n",
      "2902    /hdd/data/radioterapia/ciolaplata/2017/1.3.6.1...\n",
      "2850    /hdd/data/radioterapia/ciolaplata/2017/1.3.6.1...\n",
      "Name: dicom_full_filepath, dtype: object\n",
      "\n",
      "fold=3, train size=1271, validation size=317\n",
      "Writing TF records to /hdd/data/radioterapia/ciolaplata/tfds.2019-2018-2017.localnorm.DS10%.23456.gammaGT95.unsersampled.fold3\n",
      ">> Image 1271/1271 with 4190 bytes in shard 0 converted\n",
      ">> Image 317/317 with 4459 bytes in shard 0 converted\n",
      "Finished converting the dataset in 4.42 seconds.\n",
      "first 5 filenames from training and validation:\n",
      "2384    /hdd/data/radioterapia/ciolaplata/2017/1.3.6.1...\n",
      "2116    /hdd/data/radioterapia/ciolaplata/2017/1.3.6.1...\n",
      "1613    /hdd/data/radioterapia/ciolaplata/2017/1.3.6.1...\n",
      "2088    /hdd/data/radioterapia/ciolaplata/2017/1.3.6.1...\n",
      "2342    /hdd/data/radioterapia/ciolaplata/2017/1.3.6.1...\n",
      "Name: dicom_full_filepath, dtype: object\n",
      "4996    /hdd/data/radioterapia/ciolaplata/2018/1.3.6.1...\n",
      "2482    /hdd/data/radioterapia/ciolaplata/2017/1.3.6.1...\n",
      "4102    /hdd/data/radioterapia/ciolaplata/2019/1.3.6.1...\n",
      "2623    /hdd/data/radioterapia/ciolaplata/2017/1.3.6.1...\n",
      "4066    /hdd/data/radioterapia/ciolaplata/2019/1.3.6.1...\n",
      "Name: dicom_full_filepath, dtype: object\n",
      "\n",
      "fold=4, train size=1271, validation size=317\n",
      "Writing TF records to /hdd/data/radioterapia/ciolaplata/tfds.2019-2018-2017.localnorm.DS10%.23456.gammaGT95.unsersampled.fold4\n",
      ">> Image 1271/1271 with 4190 bytes in shard 0 converted\n",
      ">> Image 317/317 with 4681 bytes in shard 0 converted\n",
      "Finished converting the dataset in 4.50 seconds.\n"
     ]
    }
   ],
   "source": [
    "if not ARG_CROSS_VALIDATION_ENABLED:\n",
    "    do_build_simple_partition()\n",
    "else:\n",
    "    do_build_cv_partition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aPM_W5N2mJIH",
    "outputId": "b26130e5-c5fb-49bd-b191-2d4bcc785f76"
   },
   "outputs": [],
   "source": [
    "if isGoogleColab():\n",
    "    drive.flush_and_unmount()\n",
    "    print('All changes made in this colab session should now be visible in Drive.')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "tf2_create_tfrecords.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
